\newcommand{\defn}\emph

%c) v úvodní části
%i) přehled současného stavu dané vědní problematiky (s odkazy na
%literaturu) a
%ii) cíle disertace,
%d) v těle práce vlastní výsledky doktoranda s odkazy na jeho publikace v členění
%dle písm. f),
%Má-li práce formu komentovaného souboru publikací dle čl. 1, odst.1.3, pak odst. 3.1. písm.
%c) a odst. 3.1 písm. d) jsou nahrazeny spojujícím textem rozsahu alespoň 10 stran.

\chapter{Introduction}
\glsresetall

\todo[inline]{Consider quoting Diaspora -- \enquote{knowledge mines}.}

\section{State of the Art}
\label{sec:sota}

\subsection{Reasoning in First-Order Logic}

\todo[inline]{Too technical. Make it more storylike.}
\todo[inline]{Make terminology more consistent. Representation, solving (original problem or FOL problem), introduce "task" (computational task). Witness, certificate, convince, argue, ascertain $\approx$ proof. ATP finds proof, proves conjecture (don't use "proving" in other meanings).}

During the 20th century,
concluding historical development tracing back to Aristotle's work on syllogisms,
\gls{fol} emerged as the standard formalism for foundations of mathematics, namely the set theory \cite{DBLP:journals/bsl/Ferreiros01}.\todo[author=MS]{prijde mi, ze ted ta prvni veta kulminuje necim (set theory), o cem pak vlastne vubec nebudes mluvit, coz je trosku divny}
%\todo[author=MS]{jeji zacatek zni tak trochu jako pripominani historie. Pak by se nekdo znaly mohl ptat: A kde jsou Boole, Peirce, Frege (https://plato.stanford.edu/entries/logic-firstorder-emergence/), i kdyz ty jsou prave ukotveni jeste v 19. stol}
%Nowadays, the standard way to reason about set theory uses \gls{fol}.
% What is ATP good for: https://tptp.org/Seminars/ATP/Applications/Summary.html
Thanks to its expressiveness, \gls{fol} has found use in diverse areas such as
mathematics \cite{},
software and hardware synthesis and verification \cite{},
common-sense reasoning \cite{}, and
legal reasoning \cite{}.
%These areas demonstrate the general importance of reasoning in \gls{fol}.

\todo[inline,author=MS]{celkove prvni odstavec zatim pusobi nedokoncene; drive jsi tam mel tusim "problems"/"represented in logic"/"solvers" a to smerovani mi proslo nosnejsi. Mozna muzes mit nejdriv historii, pak aplikace (jeste zkust nejake prihodit) a pak to modelovani a resice?}

A reasoning task can be represented in \gls{fol} by a set of \defn{axioms} (for example, axioms of set theory, group theory, or arithmetic; in general, any collection of \gls{fol} formulas) and a \defn{conjecture}.
The axioms and the conjecture are \gls{fol} \defn{formulas} constructed using\todo{Vague?} a common \defn{signature} -- a set of non-logical\todo{Is this term ok?} symbols (identifiers of predicates and functions).
%\footnote{Predicate symbols map tuples of elements to Boolean values, and function symbols map tuples of elements to elements.}
An \defn{interpretation} of the signature specifies semantics of each of the signature's symbols.
%The (possibly\todo{Or even necessarily?} infinitely many) interpretations that satisfy a set of axioms form the \defn{theory} of the axioms.\todo[author=MS]{"In mathematical logic, a theory (also called a formal theory) is a set of sentences in a formal language." \url{https://en.wikipedia.org/wiki/Theory\_(mathematical\_logic)}}
If the conjecture is satisfied by all the interpretations that satisfy the axioms\todo{all the models of the theory},
the axioms \defn{logically entail} the conjecture.
% Here we define "theorem" solely for "automated theorem proving" to make more sense.
We then also call the conjecture a \defn{theorem} of the \defn{theory}\todo{Define \enquote{theory} properly.} defined by the axioms.

%\Gls{fol} is an established formalism capable of representing\todo{Discuss the process of representation in more detail. Give an example.} a wide range of reasoning tasks from diverse areas such as mathematics and software verification\todo{Add more examples.}.
%A \gls{fol} representation of a problem consists of a set of \defn{axioms} and a \defn{conjecture},
%each expressed as a \gls{fol} formula.
%The axioms \defn{logically entail} the conjecture if and only if\todo{Informalize -- use "if" or something similar.}
%every interpretation\todo{unclear term}\todo{Here we implicitly assume that all variables are bound. Otherwise, we would need to talk about valuation too.} that satisfies all the axioms also satisfies the conjecture\footnote{Note that if the conjecture is logically entailed by an empty set of axioms
%(that is, satisfied by every interpretation),
%the conjecture is said to be \defn{logically valid} (tautological).}.
%Solving the problem amounts to verifying that such entailment holds.\todo{Storify, add more detail, expand.}\todo{"holds" and "entailment" are both metalogical technical terms.}
%%(or, equivalently, that the conjecture holds assuming the axioms hold).

\todo[inline]{Add a paragraph discussing informal and formal proofs?}

Proof by contradiction is a common technique of proving informally a theorem in mathematics.
A straightforward counterpart exists in formal logic --
the task of certifying
entailment is easily reduced to
certifying the
unsatisfiability of a set of formulas\todo{Can we only do this in classical logic? Should this be discussed?}:
The set of axioms $\mathcal{A}$ entails conjecture $C$\todo{Consider using standard symbols -- $\Gamma$ for axioms.} if the set of formulas $\mathcal{A} \cup \{\lnot C\}$ (where $\lnot C$ is the negation of $C$) is unsatisfiable, that is, falsified by every interpretation.
In the rest of this text, we call such set of \gls{fol} formulas a \defn{\gls{fol} problem}.

%One of the approaches of establishing such entailment is proving by contradiction.
%To prove the entailment \defn{by contradiction}\todo{Introduce: Why do we care about proving by contradiction?},
%it suffices to prove\todo{I use "prove" too often.} that the set of all axioms together with the negated conjecture is unsatisfiable\todo{Clear?}.
%We call such representation of a problem \defn{refutational}\todo{Try to use standard terminology instead. Or factor out the term "refutational problem".}.
%%This is the basis of \defn{refutational theorem proving}.

Using De-Morgan laws, subformula naming (Tseitin transformation), and Skolemization\todo{Are these exactly the components of clausification? Are they named and described correctly?},
every \gls{fol} problem can be transformed into an equisatisfiable set of universally quantified clauses (disjunctions of \gls{fo} literals) in polynomial time \cite{DBLP:books/el/RV01/NonnengartW01}.
We call such representation of the problem \defn{\gls{cnf}}
and the process leading to it \defn{clausification}\todo{Define \enquote{preprocessing}.}.

Solving a \gls{cnf} problem amounts to showing that the set of clauses is unsatisfiable.
A common way to define a proof of unsatisfiability (or, generally, entailment) is using an \defn{inference calculus} -- a set of inference rules.
A proof, then, is a sequence\todo{Finite?} of clauses,
each of which is either an input\todo{potentially unclear} clause or a clause derived from some (typically one or two) of the previous clauses by an inference rule.
If all the inference rules are \defn{sound},
then each of the clauses in the proof is a logical consequence\todo{undefined term} of the input set of clauses.
Finally, if the empty clause (a trivial contradiction) appears in the proof,
the proof certifies the unsatisfiability of the input set of clauses.
In this paradigm, the task of proving unsatisfiablity can be realized as a search for (a derivation of) the empty clause.

\defn{Complete} inference calculi are of special interest:
If an inference calculus is complete and the input clause set is unsatisfiable\todo{Does it need to be finite?},
then a finite\todo{?} proof of unsatisfiability exists and can be found, for example, by exhaustively enumerating all the clauses derivable from the input clauses.
Enumerating the derivable clauses in a way that ensures a fast derivation of the empty clause (provided the input set of clauses is unsatisfiable) is the main goal of saturation-based \gls{tping}.

%If a set of clauses is unsatisfiable, then its negation is logically valid.
%If a set of clauses is satisfiable, than the satisfying interpretation defines a counter-example to the negation of the set.

%The problem of unsatisfiability and, by reduction, entailment in \gls{fol} is semi-decidable:
%There is an algorithm that will, if the input clause set is unsatisfiable, eventually find a proof and therefore establish the unsatisfiability,
%but there is no algorithm that would determine the satisfiability of every satisfiable clause set \cite{}.
%\todo[inline]{For each algorithm that is sound and complete, there is a problem that will make the algorithm loop forever. Any alg. will necessarily not terminate on some input.}
%%This is related to the fact that to show the satisfiability, one has to prove that an interpretation with certain properties exists,
%%while this interpretation may be arbitrarily large (infinite) and complex.

%Given a sound inference calculus, the process of solving a \gls{fol} problem can be viewed as a search for the empty clause.
%The search starts by visiting the input clauses and
%each subsequent step visits a clause derivable by the inference rules from some of the clauses visited before.
%\defn{\Gls{atping}} automates such search.

\subsection{Saturation-Based Theorem Proving}

\Gls{saturation} is the state-of-the-art approach to \gls{fol} \gls{tping},
as demonstrated by the continued success of the saturation-based theorem provers such as Vampire \cite{DBLP:conf/cav/KovacsV13}, E \cite{DBLP:conf/cade/0001CV19}, and SPASS \cite{DBLP:conf/cade/WeidenbachDFKSW09} in the \gls{casc} \cite{Sut16}.

A \defn{\gls{saturation}-based \gls{tper}} organizes the search for a contradiction
(the empty clause)
by maintaining and incrementally expanding so-called \defn{processed set}\todo{Consider using a different terminology: active and passive set.} -- a set of clauses that have been used exhaustively as premises of inference rules to derive new clauses.
%deriving new facts,
%starting with the input problem expressed in \gls{cnf}.
%\Gls{cnf} is sufficiently expressive:
%Any \gls{fol} problem can be converted (\enquote{clausified}) into an equisatisfiable \gls{cnf} in polynomial time.
The search starts by initializing the processed set as empty and populating the \defn{unprocessed set} with the input clauses.
In each iteration,
a clause is selected from the unprocessed set.
This clause, referred to as the \defn{given clause}, is moved into the processed set.
Then, all the clauses in the processed set are considered as premises of all the inference rules.
%Then, all the admissible inferences, in which all premises are in the processed set and at least one premise is the given clause, are performed.
All new clauses derived by the inferences are added to the unprocessed set.\todo{Consider mentioning forward and backward subsumption.}

\todo[inline]{Calculi: ordered resolution (without equality, with \gls{to}), superposition.

Vampire implements ordered resolution and superposition calculus.}

The inference calculus, that is the set of inference rules implemented by the prover, forms the core of the reasoning procedure.
The most commonly used calculi are the resolution calculus \cite{} and the superposition calculus \cite{}\todo[author=MS]{chtel jsem hned chtit, abys doplnil "for natively dealing with equality", ale k tomu se pak dostavas hned brzy dal, tak ted uz nevim}.
While the resolution calculus suffices to ensure completeness\todo{Really? Should we mention axiomatization of equality?} of the proof search\todo[author=MS]{moc nemam rad "proof search", protoze to nic neznamane}\todo[author=MS]{hlavni problem to ale je, ze normalne rikame/dokazujeme "a calculus is/isn't complete"},
additional inference rules, in practice, greatly increase the power of the prover.
For example, the superposition rule is the standard technique\todo[author=MS]{stylisticky mi trochu nesedi "inference rule IS a technique"} for reasoning with equality \cite{}.\todo[author=MS]{technicky potrebujes v superposition (jako kalkulus) nejen superposition (jako rule), ale i equality factoring a equality resolution (jako dalsi rules)}

% Superposition is a generalization of Knuth-Bendix completion.
% Superposition on UEQ is unfailing KB completion.

\todo[inline]{The calculi are normally tweaked to be minimal yet ensure completeness.}
\todo[inline]{Bare, unrestricted rules are *prolific*.

In the past, people tried to make the calculus as small as possible and yet preserve completeness.}

Adding generating rules into the calculus increases the power of each inference step at the cost of increasing the branching factor of the proof search.
Even with only the resolution inference, the size of the unprocessed set typically grows quadratically in the number of iterations \cite{}.\todo{Really? Cite a source. Or avoid discussing this.}\todo{Something like that happens in practice.}
%Even with only the resolution inference, the number of new clauses derived in each iteration may grow quadratically \cite{}.
This leads to a steadily increasing computational cost of each iteration,
which effectively slows the proof search down in the later stages.

\todo[inline]{The calculus is sound. Restrictions are included in the calculus. They are typically completeness-preserving.

For example, superposition and paramodulation only differ in the restrictions.}

To combat such slowdown, the successful calculi (notably, the ordered resolution and the superposition calculi) restrict the inferences in various ways.
For example, the superposition calculus is parameterized by a \defn{literal selection function} -- a function that selects a subset of literals in each clause.
The inferences are only performed on the selected literals.
If the selection function is well-behaved,
i.e., it selects either a negative literal or all maximal literals\todo{What does this mean? What if there are two incomparable literals in the clause? How does the selection deal with negation?},
the restriction preserves completeness of the calculus \cite{DBLP:journals/logcom/BachmairG94}.
Moreover, it may even be beneficial to restrict the inferences in a way that forfeits completeness
in exchange for a more narrow and, presumably, better-aimed\todo{Find a better expression. Alternative: \enquote{faster-converging}} proof search.

In summary, two main choice points guide the \gls{saturation}-based proof search:
\begin{enumerate}
\item Clause selection: Which of the unprocessed clauses should be selected?\todo{Explain why this is important. Typically the prover derives a lot of useless clauses. If it always selected a useful clause, it would find a proof quickly. Add another paragraph about clause selection and "proof".}
\item Inference restriction: Which of the inferences should be performed?\todo{This is affected by \gls{to} (literal selection, equality orienting) and forward subsumption.}\todo{Precedences actually likely work by pruning the search space rather than guidance.}
\end{enumerate}
%My research has dealt with both of these:
%See \cref{sec:contrib:ClauseSelection} and \cref{sec:contrib:SymbolPrecedenceRecommenders}, respectively.

\subsection{Machine Learning for Theorem Proving}

The choice points described in the previous section are typically resolved by \defn{heuristics},
that is, procedures that were designed to optimize the performance of the prover on a class of input problems.
Many heuristics are parameterized and their configuration can be tuned to optimize the performance.
A promising approach to automated optimization of the heuristics is based on learning from proof search traces.
Standard techniques of \gls{ml} are applicable.\todo{Such formulation undermines the challenge. We should stress we need non-standard techniques. Remember we often used proxy tasks. Another challenge: ML uses real vectors while we use discrete stuff, for example representations of formulas (graphs).}
For example, clause selection can be cast as a supervised classification problem \cite{}.

\Gls{ml} has been successfully applied to improve the performance of various \glspl{atper} \cite{DBLP:journals/corr/abs-2403-04017}.\todo{Undermines novelty. It is an open area.}
Specifically, \gls{ml} has been used to improve axiom selection \cite{}, clause selection \cite{}, ...
My work adds more insights to this body of research.

\todo[inline]{Explain what \gls{ml} is.}
\todo[inline]{Explain GNN. Relate to the challenge of representing the logic formulas for NN.

Consider including an overview of proxy tasks.}

\todo[inline]{Finish this section.}

\section{Contributions}

\todo[inline]{Maybe: Add an introductory sentence that ties this section to the previous one.}
%In summary, I have substantially contributed to the design, implementation, and evaluation of several \acrshort{ml}-based systems that improve the performance of the state-of-the-art \gls{atper} \gls{vampire} \cite{DBLP:conf/cav/KovacsV13}.
% on the \gls{fol} fragment of \gls{tptp}.
While the experiments have been performed using \gls{vampire},
the core techniques are applicable to any \gls{superposition}-based \gls{atper}.
Furthermore, they are potentially useful in other areas of parameterized problem solving, as detailed below.

I used the \gls{fol} fragment of \gls{tptp} as the target\todo{Consider factoring out "target".} problem distribution\todo{Replace or clarify?} of my research.
Since \gls{tptp} represents several target domains of \gls{atping},
such as mathematics and software verification,
and since \gls{vampire} represents the state of the art in \gls{atping}\todo{Ensure this is justified somewhere.},
the results can be understood as pushing the limits of \gls{fol} \gls{atping} in general.
I focused on \gls{fol} \gls{atping} because this area is well established yet nontrivial.\todo{Write a better justification.}

In all cases, the \gls{ml} is based on learning from proof searches of the target prover (\gls{vampire}) performed on problems from the target domain (\gls{tptp}).

\subsection{Symbol Precedence Recommenders}
\label{sec:contrib:SymbolPrecedenceRecommenders}

Vampire uses a \gls{to} to restrict the proof search without compromising completeness.
The ordering orients some of the equations, and the superposition inferences only apply such equations for rewriting in one direction.
Moreover, literal selection restricts the inferences on any given clause to a subset of literals, and the term ordering affects which literals are selected\todo{How? For example, can only a strictly maximal literal be selected?}.
In these two ways, the \gls{to} prunes and guides\todo{Is "guidance" an overstatement?} the proof search.

\Gls{kbo}, the default term ordering scheme used in \gls{vampire}, is parameterized by a \gls{sp}\todo{Define here or before.}.
Choosing a good \gls{sp} improves the proof search pruning and guidance.

We explored two approaches to generating a \gls{sp} for a given input problem.
In our paper \cite{} (see \cref{sec:results:simple}), we present a \gls{PrecedenceRecommender} that sorts the symbols by a linear combination\todo{Stress the challenge of designing the proxy task. "We present a p. recommender which uses ... over syntactic features.} of their syntactic features.
In a later work (see \cref{sec:results:npr}), we build on this to train a \acrshort{gnn}-based \gls{PrecedenceRecommender}.
In both cases, we trained the recommender to generate \glspl{sp} that improve the likelihood of a successful proof search.

\todo[inline]{Explain somewhere: We learn across problems. We try to generalize across problems. }

The problems in \gls{tptp} do not share signatures\todo{Consider explaining "signature" before.},
which means ...\todo{}
This poses an unusual design challenge\todo{Clarify what the challenge is: there is no clear mapping of symbols and the signature sizes vary. Thus we use a signature-agnostic model/system.},
which eventually led us to apply techniques that have been previously explored in the field of \gls{ltr}.
Our work connects the fields of \gls{ltr} and \gls{atping} and provides, notably, an instructive description of a \acrshort{gnn}-based permutation recommender.

\subsection{Clause Selection Guidance}
\label{sec:contrib:ClauseSelection}

%In each iteration of the main loop,
%a saturation-based \gls{atper} selects one of the clauses that have been inferred so far.
%The selected clause is then added to the active clause set and all the possible inferences within the active set are performed.
%The \glspl{atper} rely on heuristics to select the (given) clause.

Given clause selection is the most important choice point in a prover.
The symbol-counting heuristic \cite{DBLP:conf/cade/SchulzM16}, which is one of the most common clause selection heuristics,
prefers selecting a clause that is small in the number of symbol and variable occurrences.
In the weighted variant of this heuristic \cite{E-manual},
each symbol contributes an amount specified by its weight,
which is a parameter of the heuristic.

In the work described in \cref{sec:results:selection},
I modified Vampire to support the weighted symbol-counting clause selection heuristic
and trained a \gls{gnn} to propose a weight for each of the symbols of the input problem.
%These weights were then used in a weighted symbol-counting clause selection heuristic in \gls{vampire}.
Thanks to the use of a \gls{gnn}, this symbol weight recommender is signature-agnostic,
which solves the challenge of misaligned signatures present in \gls{tptp}.
The trained heuristic solved \SI{6.6}{\percent} more problems than the baseline.

%The resulting system solved \SI{6.6}{\percent} more problems than the baseline,
%which is an improvement greater than that introduced, under the same conditions, by AVATAR,
%a powerful proving technique fusing a SAT solver with FOL ATP.
%Our experiments revealed that constraining the training to assign a weight of at least 1 to each symbol is a fruitful strategy\todo{Disambiguate from the term "strategy"?}.
%The trained system assigned a relatively low weight to the variable occurrences,
%which matches the intuition that general clauses should be preferred to concrete ones.

\subsection{Strategy Construction and Scheduling}

Prominent \glspl{atper} expose various options that control the preprocessing and the proof search.
Besides \gls{to} and clause selection,
the options configure \gls{AxiomSelection} \cite{DBLP:conf/cade/HoderV11}, subformula naming, redundancy elimination, etc.
% redundancy elimination ~ backward and forward subsumption
The configuration of all of the options of a prover constitutes a \defn{strategy}.
Different problems may require different strategies -- for example, aggressive \gls{AxiomSelection} is, in practice, necessary to solve problems with many axioms, while it rarely brings any benefit for small problems.

The research presented in \cref{sec:results:regularization} combines two important tasks in automation of parameterized problem solving and applies them to \gls{atping}.
\begin{enumerate}
\item \Gls{aac} is the task of finding a good configuration (strategy) for a distribution of problems.
\item \Gls{AlgorithmScheduling} is the task of finding, given a portfolio of algorithms, a good schedule for a distribution of problems.
\end{enumerate}
In our work, we generated a set of strong and mutually complementary strategies for the \gls{atper} \gls{vampire},
and combined these strategies into strong schedules.
To ensure good generalization of the resulting schedules, we introduced custom regularization into the scheduling process.
To the best of our knowledge, this is the first principled treatment of generalization of \gls{AlgorithmScheduling}.\todo{Polish the wording.}

\todo[inline]{Consider including a list of publications in Contributions.}
