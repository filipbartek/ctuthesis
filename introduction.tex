\newcommand{\defn}\emph

\chapter{Introduction}

%c) v úvodní části
%i) přehled současného stavu dané vědní problematiky (s odkazy na
%literaturu) a
%ii) cíle disertace,
%d) v těle práce vlastní výsledky doktoranda s odkazy na jeho publikace v členění
%dle písm. f),
%Má-li práce formu komentovaného souboru publikací dle čl. 1, odst.1.3, pak odst. 3.1. písm.
%c) a odst. 3.1 písm. d) jsou nahrazeny spojujícím textem rozsahu alespoň 10 stran.

\todo[inline]{Explain enough for the title of the thesis to be understood and instructive.}
\todo[inline]{Introduce the basic concepts: ATP, ML, search space, combinatorial explosion of proof search, heuristics, ML.}
\todo[inline]{Ideas: "ATP is important" - provers, ... MLTP is quite established (?) - AITP conference ...}
\todo[inline]{Discuss prominent successes of machine-assisted proving. Bill McCune w/ Otter - Robbins algebra problem. Sledgehammer. Software verification. Draw inspiration from Urban, Jasmin.}
\todo[inline]{Ensure the introduction reads like a story.}

\chapter{Preliminaries}
\todo[inline]{Consider naming this chapter \enquote{\Gls{sota}}.}

\section{First-order logic}

\Gls{fol}, also known as predicate logic, is a formal language that allows modeling facts and formally reasoning about them.

\section{Automatic theorem proving (ATP)}

Many problems can be defined in \gls{fol}.
Notable examples include problems in mathematics and software verification.
Solving a problem in \gls{fol} amounts to finding a proof that the conjecture necessarily follows from the premises.
A common approach is \defn{refutation-based proving}:
We search for a proof that a trivial contradiction is a logical consequence of the premises conjoined with the negated conjecture.

\Gls{atping} for \gls{fol} has been successfully applied in software verification and advanced mathematics.
Prominent successes include ...\todo{TBA}

\Gls{casc} is a prominent annual \gls{atping} competition.
In the \gls{fof} division, the provers are evaluated on a set of 500\todo{Really?} \gls{fol} theorems.
To win the competition, the prover needs to solve as many theorems as all of its competitors.

\Gls{casc} is closely related to the \gls{tptp} problem library.
A part of the evaluation problems are sampled from \gls{tptp}, and all the problems in \gls{casc} are added into \gls{tptp} after the competition for later reference.
Both \gls{casc} and the \gls{tptp} library use the \gls{tptp} language to define the problems.
The \gls{tptp} language is, in practice, the standard language for the definition of \gls{fol} problems.

\subsection{Saturation-based theorem proving}

A saturation-based \gls{atper} searches for a proof iteratively.
In each iteration, one of the unprocessed clauses is selected and moved to the active clause set.
Then, all the possible inferences within the active clause set are performed.

The process can be improved with forward and backward subsumption.

\subsection{Strategies}

A typical \gls{atper} employs several heuristics that guide the proof search.
The heuristics determine, for example, which clause is selected as the given clause, or which inferences are performed.
Furthermore, the heuristics are typically parameterized, as there is no single configuration that works the best for all problems of interest.
A configuration of all heuristics of a \gls{atper} is known as a strategy.

In the context of \gls{atping}, \defn{\gls{aac}} is the task of finding, given an input problem, a strategy that is expected to solve the problem in a short time.
In general, \gls{aac} can be applied to any parameterized solver.
Several general-purpose \gls{aac} systems have been proposed.

\subsection{Vampire}

\subsection{TPTP problem library}

\section{Machine learning (ML)}

\Gls{ml} automates the process of creating a responsive system with a useful behavior.\todo{Compare with a dictionary definition.}
In supervised \gls{ml}, such a system is trained on a dataset that consists of training examples,
each of which specifies an input to the system and a corresponding desired output (target label).

\subsection{Neural networks}

Since ..., \glspl{ann} have been a prominent paradigm in \gls{ml}.
The rise of \gls{dl} to prominence was precipitated by the success of \gls{cnn} for image classification.

\Glspl{gnn} use the idea of aggregating local aggregation of information that is at the core of \glspl{cnn}, and generalize it to arbitrary graphs.

\subsection{Algorithm configuration}

\section{Combining ATP with ML}

The proof search in an \gls{atper} is guided by several heuristics.
These heuristics are parameterized.

For example, the clause selection heuristics decides which of the passive clauses should be selected.

Black-box view: heuristic optimization

Premise selection

Clause selection

Simplification ordering on terms

\todo[inline]{Discuss related work (Enigma etc.).}

\Gls{enigma} uses \gls{ml} to train a clause selection heuristic.
First, training data is collected by running proof searches on many problem and collecting their traces.
The clauses inferred in the proof searches are labeled as positive or negative:
The clauses that contributed to the proof are labeled positive, and the remaining clauses are labeled negative.
This training data is used to train a clause selection model that prioritizes positive clauses over negative and, hopefully, generalizes to unseen clauses.

\Gls{ml} systems have been used to improve premise selection.
In this approach, the trained model is evaluated before the proof search to prune the premise set.
This way, the completeness of the proving procedure is compromised.
However, in problems with huge premise sets, some kind of premise selection is often very helpful and a smart premise selection may aid even further.

\todo[inline]{Mention automated algorithm configuration.}

\chapter{State of the art}

\chapter{Preliminaries}

\todo[inline]{Define: ML, ATP, saturation-based ATP}

\chapter{Contributions}

This dissertation thesis presents the results of my research in \gls{ml} for \gls{atping}.
In summary, I have substantially contributed to the design and evaluation of several \acrshort{ml}-based techniques that enhance the performance of the \gls{atper} \gls{vampire} on the \gls{fol} fragment of \gls{tptp}.
While the experiments have been performed with \gls{vampire},
the techniques apply to \gls{superposition}-based \glspl{atper} in general.
Furthermore, each of them can can be potentially useful in other areas of parameterized problem solving.

Since \gls{tptp} represents many target domains of \gls{atping} and since \gls{vampire} is a prominent representative of the \acrlong{sota} in \gls{atping},
the results can be understood as improvements in \gls{fol} \gls{atping} in general.
I focused on \gls{fol} \gls{atping} because this area is well established yet nontrivial.

In all cases, the \gls{ml} is based on learning from proof searches of the target prover performed on problems from the target domain.

\section{Precedence recommenders}

Two of the techniques instantiate the \gls{kbo} by generating a \gls{sp}.
This way, the proof search is restricted: The \gls{to} restricts the inferences by orienting the equations and \glslink{LiteralSelection}{selecting literals}.
The \gls{sp} is specific to the input problem:
Problem \glspl{signature}, in general, differ in size so the \gls{PrecedenceRecommender} needs to address this when generating a \gls{precedence} for the input problem.
This poses an unusual design challenge, which finally led us to relate our research to works in the field of \gls{ltr}.

The first approach I explored scores the \glspl{symbol} using a small number of simple syntactic features of the \glspl{symbol} \cite{}.
The second approach is more involved:
Before the proof search, a \gls{gnn} is evaluated on the structure and metadata of the input \gls{problem} and assigns scores to the \glspl{symbol} (see \cref{sec:results:npr}).
Finally, the symbols are sorted by their scores.

\section{Clause selection}

See \cref{sec:results:selection}.

\section{Strategy scheduling}

See \cref{sec:results:regularization}.
