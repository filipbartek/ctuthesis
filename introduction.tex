\newcommand{\defn}\emph

%c) v úvodní části
%i) přehled současného stavu dané vědní problematiky (s odkazy na
%literaturu) a
%ii) cíle disertace,
%d) v těle práce vlastní výsledky doktoranda s odkazy na jeho publikace v členění
%dle písm. f),
%Má-li práce formu komentovaného souboru publikací dle čl. 1, odst.1.3, pak odst. 3.1. písm.
%c) a odst. 3.1 písm. d) jsou nahrazeny spojujícím textem rozsahu alespoň 10 stran.

\chapter{Preliminaries [to be removed]}
\todo[inline]{Remove this chapter once its content is subsumed by \cref{sec:sota}.}

\section{First-Order Logic}

\Gls{fol}, also known as predicate logic, is a formal language that allows modeling facts and formally reasoning about them.

\section{Automatic Theorem Proving (ATP)}

Many problems can be defined in \gls{fol}.
Notable examples include problems in mathematics and software verification.
Solving a problem in \gls{fol} amounts to finding a proof that the conjecture necessarily follows from the premises.
A common approach is \defn{refutation-based proving}:
We search for a proof that a trivial contradiction is a logical consequence of the premises conjoined with the negated conjecture.

\Gls{atping} for \gls{fol} has been successfully applied in software verification and advanced mathematics.
Prominent successes include ...\todo{TBA}

\Gls{casc} is a prominent annual \gls{atping} competition.
In the \gls{fof} division, the provers are evaluated on a set of 500\todo{Really?} \gls{fol} theorems.
To win the competition, the prover needs to solve as many theorems as all of its competitors.

\Gls{casc} is closely related to the \gls{tptp} problem library.
A part of the evaluation problems are sampled from \gls{tptp}, and all the problems in \gls{casc} are added into \gls{tptp} after the competition for later reference.
Both \gls{casc} and the \gls{tptp} library use the \gls{tptp} language to define the problems.
The \gls{tptp} language is, in practice, the standard language for the definition of \gls{fol} problems.

\subsection{Saturation-Based Theorem Proving}

A saturation-based \gls{atper} searches for a proof iteratively.
In each iteration, one of the unprocessed clauses is selected and moved to the active clause set.
Then, all the possible inferences within the active clause set are performed.

The process can be improved with forward and backward subsumption.

\subsection{Strategies}

A typical \gls{atper} employs several heuristics that guide the proof search.
The heuristics determine, for example, which clause is selected as the given clause, or which inferences are performed.
Furthermore, the heuristics are typically parameterized, as there is no single configuration that works the best for all problems of interest.
A configuration of all heuristics of a \gls{atper} is known as a strategy.

In the context of \gls{atping}, \defn{\gls{aac}} is the task of finding, given an input problem, a strategy that is expected to solve the problem in a short time.
In general, \gls{aac} can be applied to any parameterized solver.
Several general-purpose \gls{aac} systems have been proposed.

\subsection{Vampire}

\subsection{TPTP Problem Library}

\section{Machine Learning (ML)}

\Gls{ml} automates the process of creating a responsive system with a useful behavior.\todo{Compare with a dictionary definition.}
In supervised \gls{ml}, such a system is trained on a dataset that consists of training examples,
each of which specifies an input to the system and a corresponding desired output (target label).

\subsection{Neural Networks}

Since ..., \glspl{ann} have been a prominent paradigm in \gls{ml}.
The rise of \gls{dl} to prominence was precipitated by the success of \gls{cnn} for image classification.

\Glspl{gnn} use the idea of aggregating local aggregation of information that is at the core of \glspl{cnn}, and generalize it to arbitrary graphs.

\subsection{Algorithm Configuration}

\section{Combining ATP with ML}

The proof search in an \gls{atper} is guided by several heuristics.
These heuristics are parameterized.

For example, the clause selection heuristics decides which of the passive clauses should be selected.

Black-box view: heuristic optimization

Premise selection

Clause selection

Simplification ordering on terms

\todo[inline]{Discuss related work (Enigma etc.).}

\Gls{enigma} uses \gls{ml} to train a clause selection heuristic.
First, training data is collected by running proof searches on many problem and collecting their traces.
The clauses inferred in the proof searches are labeled as positive or negative:
The clauses that contributed to the proof are labeled positive, and the remaining clauses are labeled negative.
This training data is used to train a clause selection model that prioritizes positive clauses over negative and, hopefully, generalizes to unseen clauses.

\Gls{ml} systems have been used to improve premise selection.
In this approach, the trained model is evaluated before the proof search to prune the premise set.
This way, the completeness of the proving procedure is compromised.
However, in problems with huge premise sets, some kind of premise selection is often very helpful and a smart premise selection may aid even further.

\todo[inline]{Mention automated algorithm configuration.}

%\chapter{State of the art}

\chapter{Introduction}
\glsresetall

\section{State of the Art}
\label{sec:sota}

Practical problems from various domains can be formulated in general formal languages,
such as predicate logic and \gls{fol}.
Deductive reasoning offers clear-cut solutions to problem instances,
and each such solution is justified by a proof.

\Gls{sat} solvers have shown great success and practical applicability in solving problems formulated in \gls{PropositionalLogic}.
This challenges the intuition that \acrshort{np}-hardness makes \gls{sat} impractically hard.

The paradigms of \gls{fol} and \gls{hol} offer higher expressive power and more compact formulations of some important classes of problems.
The price is the increased complexity of reasoning,
which culminates in the undecidability of \gls{fol} \cite{}.


Various practical problems can be formulated as statements in \gls{fol}.
%Many hard practical problems can be encoded in \gls{fol}.
Notable domains include mathematics and software verification.
This makes \glspl{atper} attractive general-purpose solvers.

\Gls{atping} deals with the task of proving theorems automatically.
An \gls{atper} is a software tool that searches for a proof of a given logical statement.
State of the art \glspl{atper}

\todo[inline]{Explain enough for the title of the thesis to be understood and instructive.}

\todo[inline]{Clarify terms: ML, ATP, FOL, TPTP, Vampire, search space, saturation-based ATP, superposition-based ATP, parameterized problem solving (?), proof search (as training data point, as process), problem (input problem), \gls{to}, superposition inference, literal selection, inference, KBO, combinatorial explosion of proof search, heuristics, strategy scheduling, strategy optimization, complementary strategies}

\todo[inline]{Ideas: "ATP is important" - provers, ... MLTP is quite established (?) - AITP conference ...}
\todo[inline]{Discuss prominent successes of machine-assisted proving. Bill McCune w/ Otter - Robbins algebra problem. Sledgehammer. Software verification. Draw inspiration from Urban, Jasmin.}
\todo[inline]{Ensure the introduction reads like a story.}

\Gls{ml} has been successfully applied to improve the performance of various \glspl{atper} \cite{DBLP:journals/corr/abs-2403-04017}.

\todo[inline]{
ATPing has solved many useful problems. It's a cool technology.

Since ATPing is hard, it uses heuristics.

The heuristics are traditionally tuned by humans. ML has been used increasingly to improve ATP performance.

How to do ML for ATP? - Proof search guidance, HPO.
}

\todo[inline]{Justify: \enquote{Vampire represents the state of the art of \gls{atping}.}}
\todo[inline]{Explain how restricting the proof search helps.}

\section{Contributions}

This dissertation thesis presents the results of my research in \gls{ml} for \gls{atping}.
In summary, I have substantially contributed to the design, implementation, and evaluation of several \acrshort{ml}-based systems that improve the performance of the state-of-the-art \gls{atper} \gls{vampire} on the \gls{fol} fragment of \gls{tptp}.
While the experiments have been performed using \gls{vampire},
the core techniques are applicable to any \gls{superposition}-based \gls{atper}.
Furthermore, they are potentially useful in other areas of parameterized problem solving, as detailed below.

Since \gls{tptp} represents several target domains of \gls{atping}
such as mathematics and software verification,
and since \gls{vampire} represents the state of the art in \gls{atping}\todo{Ensure this is justified somewhere.},
the results can be understood as pushing the limits of \gls{fol} \gls{atping} in general.
I focused on \gls{fol} \gls{atping} because this area is well established yet nontrivial.\todo{Write a better justification.}

In all cases, the \gls{ml} is based on learning from proof searches of the target prover (\gls{vampire}) performed on problems from the target domain (\gls{tptp}).

\subsection{Symbol Precedence Recommenders}

Vampire uses a \gls{to} to restrict the proof search without compromising the completeness:
The \gls{to} orients some of the equations for unidirectional rewriting with the superposition inferences,
and literal selection restricts the inferences on any given clause to a subset of literals.
\Gls{kbo}, the standard \gls{to} used in \gls{vampire}, is parameterized by a \gls{sp}.
By restricting the inferences, the precedence guides the proof search.

We explored two approaches to generating a \gls{sp} for a given input problem.
In \cref{sec:results:simple}, we trained a \gls{PrecedenceRecommender} that sorts the symbols by a linear combination of their syntactic features.
In \cref{sec:results:npr}, we build on this work to train a \acrshort{gnn}-based \gls{PrecedenceRecommender}.
In both cases, we trained the recommender to generate \glspl{sp} that likely result in a short successful proof search.

Since the problems in \gls{tptp} are, in general, signature-agnostic,
the signatures differ both in content (the semantics of the symbols) and size (the number of the symbols).
This poses an unusual design challenge,
which finally led us to apply techniques that have been previously explored in the field of \gls{ltr}.
Our work connects the fields of \gls{ltr} and \gls{atping} and provides, notably, an instructive description of a \acrshort{gnn}-based permutation recommender.

\subsection{Clause Selection Guidance}

In each iteration of the main loop,
a saturation-based \gls{atper} selects one of the clauses that have been inferred so far.
The selected clause is then added to the active clause set and all the possible inferences within the active set are performed.
The \glspl{atper} rely on heuristics to select the clause.
The symbol-counting heuristic, which happens to be one of the most popular ones,
prioritizes clauses that are small in the number of symbol and variable occurrences.
In the weighted variant of this heuristic,
each symbol contributes an amount specified by its weight,
which is a parameter of the heuristic.

In the work described in \cref{sec:results:selection},
I used a \gls{gnn} to assign a weight to each of the symbols of the input problem.
These weights were then used in a weighted symbol-counting clause selection heuristic in \gls{vampire}.

\subsection{Strategy Construction and Scheduling}

Besides \gls{to} and clause selection, the prominent \glspl{atper} contain various other parameterized heuristics,
such as premise selection, subformula naming, backward and forward subsumption, etc.
The configuration of all the heuristics of a prover constitutes a strategy of the preprocessing and proof search.
Various strategies are suitable for various problems -- for example, aggressive premise selection is, in practice, necessary to solve problems with many premises, while it does not bring any benefit for small problems.

The research presented in \cref{sec:results:regularization} combines two important tasks in automation of parameterized problem solving and applies them to \gls{atping}.
\begin{enumerate}
\item \Gls{aac} is the task of finding a good configuration (strategy) for a distribution of problems.
\item \Gls{AlgorithmScheduling} is the task of finding, given a portfolio of algorithms, a good schedule for a distribution of problems.
\end{enumerate}
In our research, we generated a set of strong and mutually complementary strategies for the \gls{atper} \gls{vampire},
and combined these strategies into strong schedules.
To ensure good generalization of the resulting schedules, we introduced custom regularization into the scheduling process.
To the best of our knowledge, this is the first principled treatment of generalization of \gls{AlgorithmScheduling}.\todo{Polish the wording.}
