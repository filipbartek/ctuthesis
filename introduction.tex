\newcommand{\defn}\emph

%c) v úvodní části
%i) přehled současného stavu dané vědní problematiky (s odkazy na
%literaturu) a
%ii) cíle disertace,
%d) v těle práce vlastní výsledky doktoranda s odkazy na jeho publikace v členění
%dle písm. f),
%Má-li práce formu komentovaného souboru publikací dle čl. 1, odst.1.3, pak odst. 3.1. písm.
%c) a odst. 3.1 písm. d) jsou nahrazeny spojujícím textem rozsahu alespoň 10 stran.

\chapter{Preliminaries [to be removed]}
\todo[inline]{Remove this chapter once its content is subsumed by \cref{sec:sota}.}

\section{Automatic Theorem Proving (ATP)}

Many problems can be defined in \gls{fol}.
Notable examples include problems in mathematics and software verification.
Solving a problem in \gls{fol} amounts to finding a proof that the conjecture necessarily follows from the premises.
A common approach is \defn{refutation-based proving}:
We search for a proof that a trivial contradiction is a logical consequence of the premises conjoined with the negated conjecture.

\Gls{atping} for \gls{fol} has been successfully applied in software verification and advanced mathematics.
Prominent successes include ...\todo{TBA}

\Gls{casc} is a prominent annual \gls{atping} competition.
In the \gls{fof} division, the provers are evaluated on a set of 500\todo{Really?} \gls{fol} theorems.
To win the competition, the prover needs to solve as many theorems as all of its competitors.

\Gls{casc} is closely related to the \gls{tptp} problem library.
A part of the evaluation problems are sampled from \gls{tptp}, and all the problems in \gls{casc} are added into \gls{tptp} after the competition for later reference.
Both \gls{casc} and the \gls{tptp} library use the \gls{tptp} language to define the problems.
The \gls{tptp} language is, in practice, the standard language for the definition of \gls{fol} problems.

\subsection{Saturation-Based Theorem Proving}

A saturation-based \gls{atper} searches for a proof iteratively.
In each iteration, one of the unprocessed clauses is selected and moved to the active clause set.
Then, all the possible inferences within the active clause set are performed.

The process can be improved with forward and backward subsumption.

\subsection{Strategies}

A typical \gls{atper} employs several heuristics that guide the proof search.
The heuristics determine, for example, which clause is selected as the given clause, or which inferences are performed.
Furthermore, the heuristics are typically parameterized, as there is no single configuration that works the best for all problems of interest.
A configuration of all heuristics of a \gls{atper} is known as a strategy.

In the context of \gls{atping}, \defn{\gls{aac}} is the task of finding, given an input problem, a strategy that is expected to solve the problem in a short time.
In general, \gls{aac} can be applied to any parameterized solver.
Several general-purpose \gls{aac} systems have been proposed.

\subsection{Vampire}

\subsection{TPTP Problem Library}

\section{Machine Learning (ML)}

\Gls{ml} automates the process of creating a responsive system with a useful behavior.\todo{Compare with a dictionary definition.}
In supervised \gls{ml}, such a system is trained on a dataset that consists of training examples,
each of which specifies an input to the system and a corresponding desired output (target label).

\subsection{Neural Networks}

Since ..., \glspl{ann} have been a prominent paradigm in \gls{ml}.
The rise of \gls{dl} to prominence was precipitated by the success of \gls{cnn} for image classification.

\Glspl{gnn} use the idea of aggregating local aggregation of information that is at the core of \glspl{cnn}, and generalize it to arbitrary graphs.

\subsection{Algorithm Configuration}

\section{Combining ATP with ML}

The proof search in an \gls{atper} is guided by several heuristics.
These heuristics are parameterized.

For example, the clause selection heuristics decides which of the passive clauses should be selected.

Black-box view: heuristic optimization

\Gls{AxiomSelection}

Clause selection

Simplification ordering on terms

\todo[inline]{Discuss related work (Enigma etc.).}

\Gls{enigma} uses \gls{ml} to train a clause selection heuristic.
First, training data is collected by running proof searches on many problem and collecting their traces.
The clauses inferred in the proof searches are labeled as positive or negative:
The clauses that contributed to the proof are labeled positive, and the remaining clauses are labeled negative.
This training data is used to train a clause selection model that prioritizes positive clauses over negative and, hopefully, generalizes to unseen clauses.

\Gls{ml} systems have been used to improve \gls{AxiomSelection}.
In this approach, the trained model is evaluated before the proof search to prune the premise set.
This way, the completeness of the proving procedure is compromised.
However, in problems with huge premise sets, some kind of \gls{AxiomSelection} is often very helpful and a smart \gls{AxiomSelection} may aid even further.

\todo[inline]{Mention automated algorithm configuration.}

%\chapter{State of the art}

\chapter{Introduction}
\glsresetall

\todo[inline]{Consider quoting Diaspora -- \enquote{knowledge mines}.}

\section{State of the Art}
\label{sec:sota}

\subsection{Reasoning in First-Order Logic}

\Gls{fol} is an established formalism capable of representing a wide range of problems from diverse areas such as mathematics and software verification\todo{Add more examples.}.
A \gls{fol} representation of a problem consists of a set of \defn{axioms} and a \defn{conjecture},
each expressed as a \gls{fol} formula.
The axioms \defn{logically entail} the conjecture if and only if
every interpretation\todo{unclear term}\todo{Here we implicitly assume that all variables are bound. Otherwise, we would need to talk about valuation too.} that satisfies all the axioms also satisfies the conjecture\footnote{Note that if the conjecture is logically entailed by an empty set of axioms
(that is, satisfied by every interpretation),
the conjecture is said to be \defn{logically valid} (tautological).}.
Solving the problem amounts proving that such entailment holds.
%(or, equivalently, that the conjecture holds assuming the axioms hold).

To prove the entailment \defn{by contradiction},
it suffices to prove that the set of all axioms together with the negated conjecture entails a contradiction.
We call such representation of a problem \defn{refuational}.
%This is the basis of \defn{refutational theorem proving}.

Every refutational \gls{fol} problem (a set of \gls{fol} formulas) can be transformed into an equisatisfiable set of clauses (disjunctions of literals) in polynomial time \cite{}.
We call such representation of the \gls{fol} problem \defn{\gls{cnf}}
and the process leading to it \defn{clausification}\todo{Define \enquote{preprocessing}.}.

Solving a refutational \gls{cnf} problem amounts to proving that the set of clauses is \defn{unsatisfiable},
that is, that every interpretation falsifies at least one of the clauses.
A common way to represent a proof of unsatisfiability is using an \defn{inference calculus},
that is, a set of inference rules.
A proof, then, is a sequence of clauses,
each of which is either an input\todo{potentially unclear} clause or a clause derived from one of the previous clauses by an inference rule, and
last of which is the empty clause (a trivial contradiction).
If all the inference rules are \defn{correct}\todo{Is this the right term? Alternatives: consistent, valid.}, then such proof establishes unsatisfiability of the input set of clauses.

%If a set of clauses is unsatisfiable, then its negation is logically valid.
%If a set of clauses is satisfiable, than the satisfying interpretation defines a counter-example to the negation of the set.

The problem of unsatisfiability and, by reduction, entailment in \gls{fol} is semi-decidable:
There is an algorithm that will, if the input clause set is unsatisfiable, eventually determine the unsatisfiability,
but there is no algorithm that would determine the satisfiability of every satisfiable clause set.
%This is related to the fact that to show the satisfiability, one has to prove that an interpretation with certain properties exists,
%while this interpretation may be arbitrarily large (infinite) and complex.

Given a correct inference calculus, the process of solving a \gls{fol} problem can be viewed as a search for the empty clause.
The search starts by visiting the input clauses and
each subsequent step visits a clause derivable by the inference rules from some of the clauses visited before.
\defn{\Gls{atping}} automates such search.

\subsection{Notes on FOL Reasoning [to be removed]}

\todo[inline]{FOL solver is a general-purpose solver. Problems from many domains can be encoded as FOL problems.}

Practical problems from various domains can be translated into general-purpose formal languages,
such as \gls{PropositionalLogic} and \gls{fol}.\todo{What are the limitations of the encodings?}\todo{Demonstrate on examples.}
Deductive reasoning\todo{Unclear that this is related to logic.} offers clear-cut solutions\todo{vague} to formalized\todo{Unclear. Formalization is conversion of a problem in logic.} problems,
and each such solution is justified by a proof.

\todo[inline]{Reasoning task $\approx$ proving validity (semantic entailment) or satisfiability (existence of a model)}

\todo[inline]{Formalization: Solution to the original problem is equivalent to the *validity* (or satisfiability, existence of model) of a FOL formula.

The complex problem is deciding whether a fomula is valid.}

\todo[inline]{FOL is not a language. FOL theorems is a language. Disambiguate throughout the text.}

\Gls{sat} solvers have shown great success and practical applicability in solving problems formulated in \gls{PropositionalLogic} \cite{DBLP:series/faia/336}.
This challenges the intuition that \acrshort{np}-hardness makes \gls{sat} impractically hard.

The \gls{fol} paradigm offers higher expressive power and thus more compact formulations,
which makes it suitable for problems in complex domains such as mathematics and software verification.\todo{Express idea: FOL prover is a general-purpose solver.}\todo{Consider mentioning \gls{hol}.}
However, the expressiveness comes at the cost of higher complexity\todo{vague} of the reasoning task.
%The complexity of \acrlong{fol} is epitomized\todo{} by the undecidability of the logic \cite{}.
Provability in \gls{fol} is undecidable.
% If the input CNF is satisfiable, Vampire may loop forever without saturating the clause set.
% If the input CNF is unsatisfiable, Vampire will necessarily find the proof in a finite number of steps.

A \gls{fol} \defn{problem} is represented by a set of \defn{axioms} and a \defn{conjecture},
each of which is a \gls{fol} sentence.
\todo[inline]{A solver for FOL is a prover. We focus on proving (refutational, entailment) because FOL is hard. Disprovers (model builders etc.) are necessarily incomplete.}
The task of an \gls{atper} is to prove that the axioms entail\todo{Unexpected, baf. "logically entail". Introduce.} the conjecture.\todo{This sentence is unexpected.}
\defn{Refutational} provers reduce this task to proving that the axioms together with the \defn{negated conjecture} are inconsistent.

\todo[inline]{"refutational complete": no need to explicitly qualify "refutational". Vampire is complete in what it does: refutational proving.}

\subsection{Saturation-Based Theorem Proving}

\Gls{saturation}-based approach is the state-of-the-art approach in \gls{fol} \gls{tping}, as demonstrated in the \gls{casc} \cite{Sut16}.\todo{Consider praising Vampire here.}
A \defn{\gls{saturation}-based \gls{tper}} organizes the search for a contradiction
(the empty clause)
by iteratively selecting clauses

deriving new facts,
starting with the input problem expressed in \gls{cnf}.
%\Gls{cnf} is sufficiently expressive:
%Any \gls{fol} problem can be converted (\enquote{clausified}) into an equisatisfiable \gls{cnf} in polynomial time.

In each iteration of a saturation-based proof search,
a clause is selected from the \defn{unprocessed set}\todo{Consider clarifying the initialization of the processed (empty) and unprocessed (input) sets.}.
This clause, referred to as the \defn{given clause}, is moved into the \defn{processed set}.
Then, all the clauses in the processed set are considered as premises of all the inference rules.
%Then, all the admissible inferences, in which all premises are in the processed set and at least one premise is the given clause, are performed.
All new clauses derived by the inferences are added to the unprocessed set.

Vampire implements ordered resolution, superposition calculus

\todo[inline]{Calculi: ordered resolution (without equality, with \gls{to}), superposition.}

The most commonly used calculi are resolution \cite{} and superposition \cite{}.

The inference calculus, that is the set of inference rules implemented by the prover, forms the core of the reasoning procedure.
While the resolution calculus suffices to ensure refutational completeness\todo{Really? Should we mention axiomatization of equality?} of the proof search,
additional inference rules, in practice, greatly increase the power of the prover.
For example, the superposition inference rule is the standard technique for reasoning with equality \cite{}.

% Superposition is a generalization of Knuth-Bendix completion.
% Superposition on UEQ is unfailing KB completion.

\todo[inline]{The calculi are normally tweaked to be minimal yet ensure completeness.}
\todo[inline]{Bare, unrestricted rules are *prolific*.

In the past, people tried to make the calculus as small as possible and yet preserve completeness.}

Adding generating rules into the calculus increases the power of each inference step at the cost of increasing the branching factor of the proof search.
Even with only the resolution inference, the number of new clauses derived in each iteration may grow quadratically \cite{}.\todo{Really? Cite a source. Or avoid discussing this.}\todo{Something like that happens in practice.}
This leads to a steadily increasing computational cost of each iteration,
which effectively slows the proof search down in the later stages.

\todo[inline]{The calculus is sound. Restrictions are included in the calculus. They are typically completeness-preserving.

For example, superposition and paramodulation only differ in the restrictions.}

To combat such slowdown, the calculus restricts the inferences in various ways.
For example, it is possible to select a subset of literals in each clause and only perform inferences on the selected literals.
If the \defn{literal selection} function is well-behaved,
i.e., it selects either a negative literal or all maximal literals\todo{What does this mean? What if there are two incomparable literals in the clause? How does the selection deal with negation?},
the restriction preserves completeness of the superposition calculus \cite{DBLP:journals/logcom/BachmairG94}.
Moreover, it may even be beneficial to restrict the inferences in a way that forfeits completeness
in exchange for a more narrow and, presumably, better-aimed\todo{Find a better expression. Alternative: \enquote{faster-converging}} proof search.

In summary, two main choice points guide the \gls{saturation}-based proof search:
\begin{enumerate}
\item Clause selection: Which of the unprocessed clauses should be selected?\todo{Explain why this is important. Typically the prover derives a lot of useless clauses. If it always selected a useful clause, it would find a proof quickly. Add another paragraph about clause selection and "proof".}
\item Inference restriction: Which of the inferences should be performed?\todo{This is affected by \gls{to} (literal selection, equality orienting) and forward subsumption.}\todo{Precedences actually likely work by pruning the search space rather than guidance.}
\end{enumerate}
%My research has dealt with both of these:
%See \cref{sec:contrib:ClauseSelection} and \cref{sec:contrib:SymbolPrecedenceRecommenders}, respectively.

\subsection{Machine Learning for Theorem Proving}

The choice points described in the previous section are typically resolved by \defn{heuristics},
that is, procedures that were instantiated\todo{Disambiguate from logical "instantiation". Consider drawing terminology from Hoos - programming by configuration.}, by hand or automatically, to optimize the performance of the prover on a class of input problems.
A promising approach to automated optimization of the heuristics is based on learning from proof searches.
Standard techniques of \gls{ml} are applicable.\todo{Such formulation undermines the challenge. We should stress we need non-standard techniques. Remember we often used proxy tasks. Another challenge: ML uses real vectors while we use discrete stuff, for example representations of formulas (graphs).}
For example, clause selection can be cast as a supervised classification problem \cite{}.

\Gls{ml} has been successfully applied to improve the performance of various \glspl{atper} \cite{DBLP:journals/corr/abs-2403-04017}.\todo{Undermines novelty. It is an open area.}
Specifically, \gls{ml} has been used to improve axiom selection \cite{}, clause selection \cite{}, ...
My work adds more insights to this body of research.

\todo[inline]{Explain what \gls{ml} is.}
\todo[inline]{Explain GNN. Relate to the challenge of representing the logic formulas for NN.

Consider including an overview of proxy tasks.}

\todo[inline]{Finish this section.}

\subsection{Notes}

\todo[inline]{Clarify terms: TPTP, Vampire, search space, superposition-based ATP, parameterized problem solving (?), proof search (as training data point, as process), problem (input problem), \gls{to}, superposition inference, literal selection, inference, KBO, combinatorial explosion of proof search, heuristics, strategy scheduling, strategy optimization, complementary strategies}

\todo[inline]{Ideas: "ATP is important" - provers, ... MLTP is quite established (?) - AITP conference ...}
\todo[inline]{Discuss prominent successes of machine-assisted proving. Bill McCune w/ Otter - Robbins algebra problem. Sledgehammer. Software verification. Draw inspiration from Urban, Jasmin.}
\todo[inline]{Ensure the introduction reads like a story.}

\todo[inline]{
ATPing has solved many useful problems. It's a cool technology.

Since ATPing is hard, it uses heuristics.

The heuristics are traditionally tuned by humans. ML has been used increasingly to improve ATP performance.

How to do ML for ATP? - Proof search guidance, HPO.
}

\todo[inline]{Justify: \enquote{Vampire represents the state of the art of \gls{atping}.} Justify by CASC report etc.}

\section{Contributions}

\todo[inline]{Maybe: Add an introductory sentence that ties this section to the previous one.}
%In summary, I have substantially contributed to the design, implementation, and evaluation of several \acrshort{ml}-based systems that improve the performance of the state-of-the-art \gls{atper} \gls{vampire} \cite{DBLP:conf/cav/KovacsV13}.
% on the \gls{fol} fragment of \gls{tptp}.
While the experiments have been performed using \gls{vampire},
the core techniques are applicable to any \gls{superposition}-based \gls{atper}.
Furthermore, they are potentially useful in other areas of parameterized problem solving, as detailed below.

I used the \gls{fol} fragment of \gls{tptp} as the target\todo{Consider factoring out "target".} problem distribution\todo{Replace or clarify?} of my research.
Since \gls{tptp} represents several target domains of \gls{atping},
such as mathematics and software verification,
and since \gls{vampire} represents the state of the art in \gls{atping}\todo{Ensure this is justified somewhere.},
the results can be understood as pushing the limits of \gls{fol} \gls{atping} in general.
I focused on \gls{fol} \gls{atping} because this area is well established yet nontrivial.\todo{Write a better justification.}

In all cases, the \gls{ml} is based on learning from proof searches of the target prover (\gls{vampire}) performed on problems from the target domain (\gls{tptp}).

\subsection{Symbol Precedence Recommenders}
\label{sec:contrib:SymbolPrecedenceRecommenders}

Vampire uses a \gls{to} to restrict the proof search without compromising completeness.
The ordering orients some of the equations, and the superposition inferences only apply such equations for rewriting in one direction.
Moreover, literal selection restricts the inferences on any given clause to a subset of literals, and the term ordering affects which literals are selected\todo{How? For example, can only a strictly maximal literal be selected?}.
In these two ways, the \gls{to} prunes and guides\todo{Is "guidance" an overstatement?} the proof search.

\Gls{kbo}, the default term ordering scheme used in \gls{vampire}, is parameterized by a \gls{sp}\todo{Define here or before.}.
Choosing a good \gls{sp} improves the proof search pruning and guidance.

We explored two approaches to generating a \gls{sp} for a given input problem.
In our paper \cite{} (see \cref{sec:results:simple}), we present a \gls{PrecedenceRecommender} that sorts the symbols by a linear combination\todo{Stress the challenge of designing the proxy task. "We present a p. recommender which uses ... over syntactic features.} of their syntactic features.
In a later work (see \cref{sec:results:npr}), we build on this to train a \acrshort{gnn}-based \gls{PrecedenceRecommender}.
In both cases, we trained the recommender to generate \glspl{sp} that improve the likelihood of a successful proof search.

\todo[inline]{Explain somewhere: We learn across problems. We try to generalize across problems. }

The problems in \gls{tptp} do not share signatures\todo{Consider explaining "signature" before.},
which means ...\todo{}
This poses an unusual design challenge\todo{Clarify what the challenge is: there is no clear mapping of symbols and the signature sizes vary. Thus we use a signature-agnostic model/system.},
which eventually led us to apply techniques that have been previously explored in the field of \gls{ltr}.
Our work connects the fields of \gls{ltr} and \gls{atping} and provides, notably, an instructive description of a \acrshort{gnn}-based permutation recommender.

\subsection{Clause Selection Guidance}
\label{sec:contrib:ClauseSelection}

%In each iteration of the main loop,
%a saturation-based \gls{atper} selects one of the clauses that have been inferred so far.
%The selected clause is then added to the active clause set and all the possible inferences within the active set are performed.
%The \glspl{atper} rely on heuristics to select the (given) clause.

Given clause selection is the most important choice point in a prover.
The symbol-counting heuristic \cite{DBLP:conf/cade/SchulzM16}, which is one of the most common clause selection heuristics,
prefers selecting a clause that is small in the number of symbol and variable occurrences.
In the weighted variant of this heuristic \cite{E-manual},
each symbol contributes an amount specified by its weight,
which is a parameter of the heuristic.

In the work described in \cref{sec:results:selection},
I modified Vampire to support the weighted symbol-counting clause selection heuristic
and trained a \gls{gnn} to propose a weight for each of the symbols of the input problem.
%These weights were then used in a weighted symbol-counting clause selection heuristic in \gls{vampire}.
Thanks to the use of a \gls{gnn}, this symbol weight recommender is signature-agnostic,
which solves the challenge of misaligned signatures present in \gls{tptp}.
The trained heuristic solved \SI{6.6}{\percent} more problems than the baseline.

%The resulting system solved \SI{6.6}{\percent} more problems than the baseline,
%which is an improvement greater than that introduced, under the same conditions, by AVATAR,
%a powerful proving technique fusing a SAT solver with FOL ATP.
%Our experiments revealed that constraining the training to assign a weight of at least 1 to each symbol is a fruitful strategy\todo{Disambiguate from the term "strategy"?}.
%The trained system assigned a relatively low weight to the variable occurrences,
%which matches the intuition that general clauses should be preferred to concrete ones.

\subsection{Strategy Construction and Scheduling}

Prominent \glspl{atper} expose various options that control the preprocessing and the proof search.
Besides \gls{to} and clause selection,
the options configure \gls{AxiomSelection} \cite{DBLP:conf/cade/HoderV11}, subformula naming, redundancy elimination, etc.
% redundancy elimination ~ backward and forward subsumption
The configuration of all of the options of a prover constitutes a \defn{strategy}.
Different problems may require different strategies -- for example, aggressive \gls{AxiomSelection} is, in practice, necessary to solve problems with many axioms, while it rarely brings any benefit for small problems.

The research presented in \cref{sec:results:regularization} combines two important tasks in automation of parameterized problem solving and applies them to \gls{atping}.
\begin{enumerate}
\item \Gls{aac} is the task of finding a good configuration (strategy) for a distribution of problems.
\item \Gls{AlgorithmScheduling} is the task of finding, given a portfolio of algorithms, a good schedule for a distribution of problems.
\end{enumerate}
In our work, we generated a set of strong and mutually complementary strategies for the \gls{atper} \gls{vampire},
and combined these strategies into strong schedules.
To ensure good generalization of the resulting schedules, we introduced custom regularization into the scheduling process.
To the best of our knowledge, this is the first principled treatment of generalization of \gls{AlgorithmScheduling}.\todo{Polish the wording.}

\todo[inline]{Consider including a list of publications in Contributions.}
