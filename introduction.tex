\newcommand{\defn}\emph

%c) v úvodní části
%i) přehled současného stavu dané vědní problematiky (s odkazy na
%literaturu) a
%ii) cíle disertace,
%d) v těle práce vlastní výsledky doktoranda s odkazy na jeho publikace v členění
%dle písm. f),
%Má-li práce formu komentovaného souboru publikací dle čl. 1, odst.1.3, pak odst. 3.1. písm.
%c) a odst. 3.1 písm. d) jsou nahrazeny spojujícím textem rozsahu alespoň 10 stran.

\chapter{Preliminaries [to be removed]}
\todo[inline]{Remove this chapter once its content is subsumed by \cref{sec:sota}.}

\section{First-Order Logic}

\Gls{fol}, also known as predicate logic, is a formal language that allows modeling facts and formally reasoning about them.

\section{Automatic Theorem Proving (ATP)}

Many problems can be defined in \gls{fol}.
Notable examples include problems in mathematics and software verification.
Solving a problem in \gls{fol} amounts to finding a proof that the conjecture necessarily follows from the premises.
A common approach is \defn{refutation-based proving}:
We search for a proof that a trivial contradiction is a logical consequence of the premises conjoined with the negated conjecture.

\Gls{atping} for \gls{fol} has been successfully applied in software verification and advanced mathematics.
Prominent successes include ...\todo{TBA}

\Gls{casc} is a prominent annual \gls{atping} competition.
In the \gls{fof} division, the provers are evaluated on a set of 500\todo{Really?} \gls{fol} theorems.
To win the competition, the prover needs to solve as many theorems as all of its competitors.

\Gls{casc} is closely related to the \gls{tptp} problem library.
A part of the evaluation problems are sampled from \gls{tptp}, and all the problems in \gls{casc} are added into \gls{tptp} after the competition for later reference.
Both \gls{casc} and the \gls{tptp} library use the \gls{tptp} language to define the problems.
The \gls{tptp} language is, in practice, the standard language for the definition of \gls{fol} problems.

\subsection{Saturation-Based Theorem Proving}

A saturation-based \gls{atper} searches for a proof iteratively.
In each iteration, one of the unprocessed clauses is selected and moved to the active clause set.
Then, all the possible inferences within the active clause set are performed.

The process can be improved with forward and backward subsumption.

\subsection{Strategies}

A typical \gls{atper} employs several heuristics that guide the proof search.
The heuristics determine, for example, which clause is selected as the given clause, or which inferences are performed.
Furthermore, the heuristics are typically parameterized, as there is no single configuration that works the best for all problems of interest.
A configuration of all heuristics of a \gls{atper} is known as a strategy.

In the context of \gls{atping}, \defn{\gls{aac}} is the task of finding, given an input problem, a strategy that is expected to solve the problem in a short time.
In general, \gls{aac} can be applied to any parameterized solver.
Several general-purpose \gls{aac} systems have been proposed.

\subsection{Vampire}

\subsection{TPTP Problem Library}

\section{Machine Learning (ML)}

\Gls{ml} automates the process of creating a responsive system with a useful behavior.\todo{Compare with a dictionary definition.}
In supervised \gls{ml}, such a system is trained on a dataset that consists of training examples,
each of which specifies an input to the system and a corresponding desired output (target label).

\subsection{Neural Networks}

Since ..., \glspl{ann} have been a prominent paradigm in \gls{ml}.
The rise of \gls{dl} to prominence was precipitated by the success of \gls{cnn} for image classification.

\Glspl{gnn} use the idea of aggregating local aggregation of information that is at the core of \glspl{cnn}, and generalize it to arbitrary graphs.

\subsection{Algorithm Configuration}

\section{Combining ATP with ML}

The proof search in an \gls{atper} is guided by several heuristics.
These heuristics are parameterized.

For example, the clause selection heuristics decides which of the passive clauses should be selected.

Black-box view: heuristic optimization

Premise selection

Clause selection

Simplification ordering on terms

\todo[inline]{Discuss related work (Enigma etc.).}

\Gls{enigma} uses \gls{ml} to train a clause selection heuristic.
First, training data is collected by running proof searches on many problem and collecting their traces.
The clauses inferred in the proof searches are labeled as positive or negative:
The clauses that contributed to the proof are labeled positive, and the remaining clauses are labeled negative.
This training data is used to train a clause selection model that prioritizes positive clauses over negative and, hopefully, generalizes to unseen clauses.

\Gls{ml} systems have been used to improve premise selection.
In this approach, the trained model is evaluated before the proof search to prune the premise set.
This way, the completeness of the proving procedure is compromised.
However, in problems with huge premise sets, some kind of premise selection is often very helpful and a smart premise selection may aid even further.

\todo[inline]{Mention automated algorithm configuration.}

%\chapter{State of the art}

\chapter{Introduction}
\glsresetall

\section{State of the Art}
\label{sec:sota}

\subsection{Deductive Reasoning}

Practical problems from various domains can be formulated in general formal languages,
such as predicate logic and \gls{fol}.
Deductive reasoning offers clear-cut solutions to problem instances,
and each such solution is justified by a proof.

\Gls{sat} solvers have shown great success and practical applicability in solving problems formulated in \gls{PropositionalLogic}.
This challenges the intuition that \acrshort{np}-hardness makes \gls{sat} impractically hard.

The paradigms of \gls{fol} and \gls{hol} offer higher expressive power and more compact formulations of some important classes of problems.
The price is the increased complexity of reasoning,
which culminates in the undecidability of \gls{fol} \cite{}.

\todo[inline]{Explain: CNF, "refutational" CNF, axioms, negated conjecture, input problem}

\subsection{Saturation-Based Theorem Proving}

\Gls{saturation}-based approach has shown notable success in \gls{fol} \gls{tping}, as demonstrated in the \gls{casc} \cite{Sut16}.\todo{Consider praising Vampire here.}
A \gls{saturation}-based \gls{tper} searches for a refutation
(a trivial contradiction represented by the empty clause)
by iteratively deriving new facts,
starting with the input problem expressed in \gls{cnf}.

In each iteration, a clause is selected from the \defn{unprocessed set}.
This clause, referred to as the \defn{given clause}, is moved into the \defn{processed set}.
Then, all the clauses in the processed set are considered as premises of all the inference rules.
%Then, all the admissible inferences, in which all premises are in the processed set and at least one premise is the given clause, are performed.
Clauses derived by the generating inferences are added to the unprocessed set.

The inference calculus, that is the set of inference rules implemented by the prover, forms the core of the reasoning procedure.
While the resolution inference rule suffices to ensure the refutational completeness\todo{Really? Should we mention axiomatization of equality?} of the proof search,
additional inference rules, in practice, greatly increase the power of the prover.
For example, the superposition inference rule has demonstrated notable success in reasoning with equality \cite{}.

Adding generating rules into the calculus increases the power of each inference step at the cost of increasing the branching factor of the proof search.
Even with only the resolution inference, the number of new clauses derived in each iteration may grow exponentially \cite{}.\todo{Really? Cite a source.}\todo{Something like that happens in practice.}
This leads to a steadily increasing computational cost of each iteration,
which effectively slows the proof search down in the later stages.
To offset this slowdown, the provers restrict the inferences in various ways.
For example, it is possible to select a subset of literals in each clause and only perform inferences on the selected literals.
If the \defn{literal selection} procedure satisfies certain conditions\todo{Clarify or cite a source.},
the restriction preserves completeness of the calculus.
In some cases, it is even beneficial to forfeit the completeness in exchange for a more narrow and, presumably, faster-converging proof search.

In summary, two main choice points guide the \gls{saturation}-based proof search:
\begin{enumerate}
\item Clause selection: Which of the unprocessed clauses should be selected?
\item Inference restriction: Which of the inferences should be performed?\todo{This is affected by \gls{to} (literal selection, equality orienting) and forward subsumption.}\todo{Precedences actually likely work by pruning the search space rather than guidance.}
\end{enumerate}
%My research has dealt with both of these:
%See \cref{sec:contrib:ClauseSelection} and \cref{sec:contrib:SymbolPrecedenceRecommenders}, respectively.

\subsection{Machine Learning for Theorem Proving}

While refutationally-complete calculi are in common use in \gls{atping},
their practical utility is limited by the high branching factor of the proof search.
In practice, the proof search with a complete calculus often slows down quickly and consumes an ever-increasing amount of memory,
so we need ways to find a proof fast if such a proof exists.

The standard approaches to proof search guidance rely on heuristics.
Such heuristics are tweaked by human experts based on experience with solving problems from various domains.
It is also possible to configure the heuristics automatically using \gls{ml}.

\todo[inline]{Finish this section.}

\subsection{Notes}

Various practical problems can be formulated as statements in \gls{fol}.
%Many hard practical problems can be encoded in \gls{fol}.
Notable domains include mathematics and software verification.
This makes \glspl{atper} attractive general-purpose solvers.

\Gls{atping} deals with the task of proving theorems automatically.
An \gls{atper} is a software tool that searches for a proof of a given logical statement.
State of the art \glspl{atper}

\todo[inline]{Explain enough for the title of the thesis to be understood and instructive.}

\todo[inline]{Clarify terms: ML, ATP, FOL, TPTP, Vampire, search space, saturation-based ATP, superposition-based ATP, parameterized problem solving (?), proof search (as training data point, as process), problem (input problem), \gls{to}, superposition inference, literal selection, inference, KBO, combinatorial explosion of proof search, heuristics, strategy scheduling, strategy optimization, complementary strategies}

\todo[inline]{Ideas: "ATP is important" - provers, ... MLTP is quite established (?) - AITP conference ...}
\todo[inline]{Discuss prominent successes of machine-assisted proving. Bill McCune w/ Otter - Robbins algebra problem. Sledgehammer. Software verification. Draw inspiration from Urban, Jasmin.}
\todo[inline]{Ensure the introduction reads like a story.}

\Gls{ml} has been successfully applied to improve the performance of various \glspl{atper} \cite{DBLP:journals/corr/abs-2403-04017}.

\todo[inline]{
ATPing has solved many useful problems. It's a cool technology.

Since ATPing is hard, it uses heuristics.

The heuristics are traditionally tuned by humans. ML has been used increasingly to improve ATP performance.

How to do ML for ATP? - Proof search guidance, HPO.
}

\todo[inline]{Justify: \enquote{Vampire represents the state of the art of \gls{atping}.}}
\todo[inline]{Explain how restricting the proof search helps.}

\section{Contributions}

\todo[inline]{Maybe: Add an introductory sentence that ties this section to the previous one.}
In summary, I have substantially contributed to the design, implementation, and evaluation of several \acrshort{ml}-based systems that improve the performance of the state-of-the-art \gls{atper} \gls{vampire}.
% on the \gls{fol} fragment of \gls{tptp}.
While the experiments have been performed using \gls{vampire},
the core techniques are applicable to any \gls{superposition}-based \gls{atper}.
Furthermore, they are potentially useful in other areas of parameterized problem solving, as detailed below.

I used the \gls{fol} fragment of \gls{tptp} as the target problem distribution\todo{Replace or clarify?} of my research.
Since \gls{tptp} represents several target domains of \gls{atping}
such as mathematics and software verification,
and since \gls{vampire} represents the state of the art in \gls{atping}\todo{Ensure this is justified somewhere.},
the results can be understood as pushing the limits of \gls{fol} \gls{atping} in general.
I focused on \gls{fol} \gls{atping} because this area is well established yet nontrivial.\todo{Write a better justification.}

In all cases, the \gls{ml} is based on learning from proof searches of the target prover (\gls{vampire}) performed on problems from the target domain (\gls{tptp}).

\subsection{Symbol Precedence Recommenders}
\label{sec:contrib:SymbolPrecedenceRecommenders}

Vampire uses a \gls{to} to restrict the proof search without compromising completeness.
The ordering orients some of the equations, and the superposition inferences only apply such equations for rewriting in one direction.
Moreover, literal selection restricts the inferences on any given clause to a subset of literals, and the term ordering influences the selection\todo{How? For example, can only a strictly maximal literal be selected?}.
In these two ways, the \gls{to} prunes and guides\todo{Is "guidance" an overstatement?} the proof search.

\Gls{kbo}, the default term ordering scheme used in \gls{vampire}, is parameterized by a \gls{sp}\todo{Define here or before.}.
Choosing a good \gls{sp} improves the proof search pruning and guidance.

We explored two approaches to generating a \gls{sp} for a given input problem.
In our paper \cite{} (see \cref{sec:results:simple}), we present a \gls{PrecedenceRecommender} that sorts the symbols by a linear combination of their syntactic features.
In a later work (see \cref{sec:results:npr}), we build on this to train a \acrshort{gnn}-based \gls{PrecedenceRecommender}.
In both cases, we trained the recommender to generate \glspl{sp} that likely result in a short successful proof search.

The problems in \gls{tptp} do not share signatures.
This poses an unusual design challenge\todo{Clarify what the challenge is: there is no clear mapping of symbols and the signature sizes vary. Thus we use a signature-agnostic model/system.},
which eventually led us to apply techniques that have been previously explored in the field of \gls{ltr}.
Our work connects the fields of \gls{ltr} and \gls{atping} and provides, notably, an instructive description of a \acrshort{gnn}-based permutation recommender.

\subsection{Clause Selection Guidance}
\label{sec:contrib:ClauseSelection}

In each iteration of the main loop,
a saturation-based \gls{atper} selects one of the clauses that have been inferred so far.
The selected clause is then added to the active clause set and all the possible inferences within the active set are performed.
The \glspl{atper} rely on heuristics to select the clause.
The symbol-counting heuristic, which happens to be one of the most popular ones,
prioritizes clauses that are small in the number of symbol and variable occurrences.
In the weighted variant of this heuristic,
each symbol contributes an amount specified by its weight,
which is a parameter of the heuristic.

In the work described in \cref{sec:results:selection},
I used a \gls{gnn} to assign a weight to each of the symbols of the input problem.
These weights were then used in a weighted symbol-counting clause selection heuristic in \gls{vampire}.

\subsection{Strategy Construction and Scheduling}

Besides \gls{to} and clause selection, the prominent \glspl{atper} contain various other parameterized heuristics,
such as premise selection, subformula naming, backward and forward subsumption, etc.
The configuration of all the heuristics of a prover constitutes a strategy of the preprocessing and proof search.
Various strategies are suitable for various problems -- for example, aggressive premise selection is, in practice, necessary to solve problems with many premises, while it does not bring any benefit for small problems.

The research presented in \cref{sec:results:regularization} combines two important tasks in automation of parameterized problem solving and applies them to \gls{atping}.
\begin{enumerate}
\item \Gls{aac} is the task of finding a good configuration (strategy) for a distribution of problems.
\item \Gls{AlgorithmScheduling} is the task of finding, given a portfolio of algorithms, a good schedule for a distribution of problems.
\end{enumerate}
In our research, we generated a set of strong and mutually complementary strategies for the \gls{atper} \gls{vampire},
and combined these strategies into strong schedules.
To ensure good generalization of the resulting schedules, we introduced custom regularization into the scheduling process.
To the best of our knowledge, this is the first principled treatment of generalization of \gls{AlgorithmScheduling}.\todo{Polish the wording.}
