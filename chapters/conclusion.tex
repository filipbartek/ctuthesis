\chapter{Conclusion}

%e) in the concluding part,
%i) an overview of the dissertation results including the doctoral student’s own
%original contributions (i.e. a brief summary of the original results of the
%dissertation, in what way they improve the current state-of-the-art),
%ii) the conclusions for further scientific development or for transfer the results
%to practice,

%e) v závěrečné části
%i) přehled výsledků disertace včetně původního přínosu doktoranda (tj.
%stručný přehled původních výsledků disertace, v čem zlepšují současný
%stav),
%ii) závěry pro další rozvoj vědy nebo pro realizaci v praxi,

\section{Summary}

% https://www.grammarly.com/blog/punctuation-capitalization/comma-before-because/
\Gls{ml} provides an attractive direction for improving \gls{atping} because provers are complex, heuristic-driven systems whose behavior is difficult to optimize by hand,
and because, typically, training data can be generated in large amounts, limited only by the computational resources\todo{Training problem sets are, however, often very limited.}.
In this thesis,
I presented several novel techniques of applying \gls{ml} to improve saturation-based \gls{atping} for \gls{fol}.
By using the prover \gls{vampire} and the \gls{tptp} problem library in the experiments,
I demonstrated the degree to which these techniques can be used to improve on the state of the art in \gls{atping}.

The first broad approach I explored is the configuration of a chosen heuristic on a per-problem basis:
Given an input problem, a trained recommender configures a heuristic in \gls{vampire} in order to maximize the chance that the problem is solved quickly.
I configured two heuristics in this way:
\gls{sot} (via \gls{sp}; see \cref{sec:results:simple,sec:results:npr}) and weighted symbol-counting clause selection (see \cref{sec:results:selection}).\todo{Mention that the shape of the configuration of the heuristic depends on the signature size. Standard AC approaches expect fixed shape.}

Among the \gls{ml} models\todo{Is this term appropriate? Alternatives: model families, approaches, techniques.} I experimented with,
the \glspl{gnn} were prominently successful in configuring the heuristics,
yielding an improvement of \pc{4} and \pc{6.6} over a baseline, respectively, in the expected percentage of problems solved in a fixed time limit.
Notably, using a \gls{gnn} allows training a signature-agnostic recommender -- one that learns from the structure of the training problems, in a manner invariant to symbol renaming.

Each of the \acrshort{gnn}-based recommenders is trained on a proxy task of ranking objects (precedences or clauses, respectively)
with training data in the form of ordered pairs of objects.
Notably, the designs of these recommenders demonstrate how methods proposed in the area of \acrlong{ltr}
can be applied in \gls{atping}.

The second approach (see \cref{sec:results:regularization,sec:results:cautious}) configures a large number of heuristics jointly.
Instead of recommending a single configuration on a per-problem basis,
I constructed a static schedule of complementary configurations to optimize \gls{vampire}'s performance on a whole distribution of problems.

The main innovation this branch of my research introduced is the rigorous treatment of generalization of the schedules to unseen problems.
To this effect, I proposed a regularized greedy algorithm for constructing the schedules.
This algorithm, as well as the whole idea of regularization of algorithm schedules, has potential applications in domains outside of \gls{tping}\todo{Provide some examples.}.

\todo[inline]{Highlight the data I published.}

\section{Future Work}

\todo[inline,color=green]{New section}

The research presented in this thesis,
besides demonstrating several new approaches for applying \gls{ml} in \gls{atping},
also opens new directions for future research.\todo{Finish the introductory paragraph.}

In all the experiments presented in this thesis, I used the runtime\todo{I used saturation loop iterations and CPU instructions.} as a measure of quality of an execution.
Timeouts are interpreted as executions that would never finish successfully even if executed with unlimited time, which is a common way of dealing with timeouts in the \gls{tping} community.
Instead, timeouts could be treated more rigorously and precisely by modeling the runtime of a timed-out execution by a distribution estimated from the executions that did terminate.\todo{Mention and reference research on censored data. Maybe Run2Survive.}
Similarly, we could account for noise in runtime of successful executions by modeling runtime of each execution by a distribution.\todo{Abstract this paragraph.}

\todo[inline]{%
Symbol precedence:\\
Train to predict KBO weights along with the precedence.\\
Recommend function and predicate precedences jointly.\\
}

% Constrained logistic regression
In the Neural Precedence Recommender (see \cref{sec:results:npr}),
a trained \gls{nn} predicts symbol scores and
a symbol precedence is constructed by ordering the symbols by their scores.
If, instead of using a \gls{nn}, the symbol score was expressed as a weighted sum of syntactic symbol features (such as those used in \cref{sec:results:simple} -- arity, number of occurrences, etc.),
the loss I used to train the \gls{nn} would be a convex function of the weights of the symbol features.
This would allow finding nearly optimal feature weights efficiently \cite{},
and the resulting weights, along with the simple algorithm that produces the precedence by ordering the symbols by their scores,
could be integrated in the \gls{tper} easily.
The disadvantage of this approach is the significantly reduced capacity\todo{Reference to the technical term \enquote{model capacity}} in customizing the symbol weights to the particular input problem.
It remains unclear whether this reduced capacity would translate into a significant reduction of performance.

Similarly, a clause weight model trainable by convex optimization could be used for clause selection (see \cref{sec:results:selection}):
Instead of estimating the weight of each symbol dynamically by a \gls{nn} for each input problem,
we could optimize static weights for (possibly overlapping) categories of symbols,
such as
\enquote{predicate symbol} (capturing the number of literals in a clause)\todo{Doesn't make sense.},
\enquote{predicate symbol of arity 1},
\enquote{Skolem symbol}, etc.
Such generic symbol-counting clause features could be easily augmented by other clause features,
such as term depth, number of bound variables, derivation tree depth and size, or derivation distance from a goal clause.\todo{Reference MS's research on the generalization of age-based selection.}

Furthermore, in a problem domain with a shared signature, such as Mizar Mathematical Library \cite{}\todo{Reference MPTP 0.2 (2016) as well. Consider also referencing Isabelle and Seventeen.},
the scores or weights of individual symbols could be optimized directly.

The clause selection heuristic is trained on pairs of clauses.
From each prover execution, each proof clause is paired with each non-proof selected\todo{or active?} clause.
We could make the training more precise by only pairing clauses that co-habited the unprocessed set during the proof search.\todo{Reference research that does this. See KC's paper with IProver.}

For practical reasons, I trained the clause selection heuristic without AVATAR. ...

\todo[inline]{%
Clause selection:\\
Generalize to RNN on term structure.\\
Generate data with more Vampire configurations, for example with AVATAR.\\
}

\todo[inline]{%
Strategies and schedules:\\
Greedy schedule algorithm: Analyze in more detail. Figure out on what kind of data it works close to optimum.\\
Dynamic scheduling.\\
Better schedule specialization\\
Regularization: Find more regularizers. Regularize ...\\
Optimize parallel schedules.\\
}

\todo[inline]{%
General future work:\\
Automate saturation-based proving more\\
Apply in domains outside logic\\
}

\todo[inline,color=red]{Finish.}
