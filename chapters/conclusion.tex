\chapter{Conclusion}

%e) in the concluding part,
%i) an overview of the dissertation results including the doctoral student’s own
%original contributions (i.e. a brief summary of the original results of the
%dissertation, in what way they improve the current state-of-the-art),
%ii) the conclusions for further scientific development or for transfer the results
%to practice,

%e) v závěrečné části
%i) přehled výsledků disertace včetně původního přínosu doktoranda (tj.
%stručný přehled původních výsledků disertace, v čem zlepšují současný
%stav),
%ii) závěry pro další rozvoj vědy nebo pro realizaci v praxi,

\section{Summary}

% https://www.grammarly.com/blog/punctuation-capitalization/comma-before-because/
\Gls{ml} provides an attractive direction for improving \gls{atping} because provers are complex, heuristic-driven systems whose behavior is difficult to optimize by hand,
and because, typically, training data can be generated in large amounts, limited only by the computational resources\todo{Training problem sets are, however, often very limited.}.
In this thesis,
I presented several novel techniques of applying \gls{ml} to improve saturation-based \gls{atping} for \gls{fol}.
By using the prover \gls{vampire} and the \gls{tptp} problem library in the experiments,
I demonstrated the degree to which these techniques can be used to improve on the state of the art in \gls{atping}.

The first broad approach I explored is the configuration of a chosen heuristic on a per-problem basis:
Given an input problem, a trained recommender configures a heuristic in \gls{vampire} in order to maximize the chance that the problem is solved quickly.
I configured two heuristics in this way:
\gls{sot} (via \gls{sp}; see \cref{sec:results:simple,sec:results:npr}) and weighted symbol-counting clause selection (see \cref{sec:results:selection}).\todo{Mention that the shape of the configuration of the heuristic depends on the signature size. Standard AC approaches expect fixed shape.}

Among the \gls{ml} models\todo{Is this term appropriate? Alternatives: model families, approaches, techniques.} I experimented with,
the \glspl{gnn} were prominently successful in configuring the heuristics,
yielding an improvement of \pc{4} and \pc{6.6} over a baseline, respectively, in the expected percentage of problems solved in a fixed time limit.
Notably, using a \gls{gnn} allows training a signature-agnostic recommender -- one that learns from the structure of the training problems, in a manner invariant to symbol renaming.

Each of the \acrshort{gnn}-based recommenders is trained on a proxy task\todo[color=yellow]{Emphasize. Present the concept as a contribution.} of ranking objects (precedences or clauses, respectively)
with training data in the form of ordered pairs of objects.
Notably, the designs of these recommenders demonstrate how methods proposed in the area of \acrlong{ltr}
can be applied in \gls{atping}.

The second approach (see \cref{sec:results:regularization,sec:results:cautious}) configures a large number of heuristics jointly.
Instead of recommending a single configuration on a per-problem basis,
I constructed a static schedule of complementary configurations to optimize \gls{vampire}'s performance on a whole distribution of problems.

The main innovation this branch of my research introduced is the rigorous treatment of generalization of the schedules to unseen problems.
To this effect, I proposed a regularized greedy algorithm for constructing the schedules.
%This algorithm, as well as the whole idea of regularization of algorithm schedules, has clear potential applications in domains outside of \gls{tping}\todo{Provide some examples.}.

\todo[inline]{Highlight the data I published.}

\section{Future Work}
% Rule of thumb: Only mention stuff that makes sense in the context of Summary.

\todo[inline]{Mention some steps that synthesize. Look behind the horizon. Make this section sound visionary.}

%The research presented in this thesis,
%besides demonstrating several new approaches for applying \gls{ml} in \gls{atping},
%also opens new directions for future research.\todo{Finish the introductory paragraph.}

Various models and associated training algorithms have been proposed as approaches to \gls{ml}.
%Each of them is associated with specific strengths and weaknesses.
%Each of these models can be applied in various tasks of fitting type.
In my research, I used \glspl{nn} trained by \acrlong{sgd}, Elastic-Net (regularized linear regression), and gradient boosted decision trees\todo[color=yellow]{Unify terminology with the respective section in Results.}.
The \glspl{nn} trained on pairwise ranking proxy tasks\todo[color=yellow]{Ensure that the concept of \enquote{proxy task} is known from the Summary.} demonstrated the greatest success in optimization of the prover heuristics (see \cref{sec:results:npr,sec:results:selection}).
%Other models might allow reaching greater performance, training efficiency, or ease of deployment.
%Simple, low-capacity models such as the linear model could be trained and deployed more easily at a performance cost that remains unknown.
Different attractive tradeoffs between \gls{tping} performance, training efficiency, ease of deployment, and explainability might be achieved by using different models or proxy tasks.

In each of the problem domains that admit encoding in \gls{fol},
such as mathematics (formalized, for example, in Mizar Mathematical Library \cite{DBLP:journals/jfrea/GrabowskiKN10,DBLP:journals/jar/Urban06} or Archive of Formal Proofs \cite{DBLP:conf/mkm/BlanchetteHMN15,DBLP:conf/itp/DesharnaisVBW22}) or software verification,
a common set of symbols -- a shared signature -- can be identified.
Configuring \gls{sot} or \gls{ClauseSelection}
for such a domain could be tackled by combining relevant parts of my research with new approaches suitable for working with a fixed signature.
For example, convex optimization could be used to optimize the symbol scores and weights down to the global optimum with respect to the pairwise ranking loss function.\todo{too technical}

While saturation-based proving is well established and researched in the context of \gls{fol},
the paradigm, with appropriate modifications, has also been recently successfully applied to \gls{hol} \cite{DBLP:journals/ki/Steen20,DBLP:journals/jar/VukmirovicBBCNT22,DBLP:conf/ijcar/BhayatS24}.
All of the heuristic optimization\todo{Ambiguous: optimization of heuristics, or optimization by heuristics?} techniques I introduced in this thesis could potentially be adapted and applied in saturation-based proving in \gls{hol}.

Highly-parameterized solvers are common in various areas of complex problem solving besides \gls{fol} and \gls{hol}.
Alternatively, two or more solvers may be available for a problem domain\todo{Disambiguate from problem domains within FOL. Maybe: Use "area".} in a setting known as \gls{as}\todo[color=yellow]{Do define the acronym "AC" if its not used anywhere else.} \cite{DBLP:journals/ai/BischlKKLMFHHLT16}.
My work on regularized strategy schedule construction (see \cref{sec:results:regularization,sec:results:cautious}) can be applied to an arbitrary parameterized solver or collection of solvers irrespective of the problem domain.
It would be particularly interesting to explore whether and how the best configuration of the regularization is influenced by the specifics of the domain.
%such as the number of solvers and training problems available, the degree of specialization of the solvers, or the density of timeouts.

\todo[inline]{Add a closing paragraph.}
