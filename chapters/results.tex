%\newcommand*{\IncludePaper}[2][height=\paperheight-1.5in]{\includepdf[pages=-, pagecommand={}, #1]{#2}}
\newcommand*{\IncludePaper}[2][height=\paperheight-1.5in]{\includepdf[pages=1, pagecommand={}, #1]{#2}}
%\newcommand*{\IncludePaper}[2][]{}

\chapter{Results}

The results of my research have been published at various conferences and workshops.
This chapter includes a selection of the most important of these publications.
Each of the publications is presented in its final published form and is, as such, self-contained.

\todo[inline]{Consider promoting the paper sections to chapters.}
\todo[inline]{Emphasize that all of the solutions use ML for preprocessing.}
\todo[inline]{Mention that the NNs can be swapped -- these results show an example of a NN.}
\todo[inline]{In each section, summarize the results of the respective paper. Introduce each paper from a contemporary perspective. Alternatively, do all the introduction in section Contributions.}

\section{Learning Precedences from Simple Symbol Features}
\label{sec:results:simple}

State-of-the-art \glspl{atper}, including \gls{vampire}, use the \gls{SuperpositionCalculus} as the core of the reasoning procedure.
The inferences of the \gls{SuperpositionCalculus} are restricted by a \gls{sot}.
Various schemes are used to instantiate the ordering,
\gls{kbo} and \gls{lpo} being among the most popular ones.
Both of these \gls{to} schemes are parameterized by a \gls{sp} -- a permutation of the symbols in the signature.

The standard methods implemented in \gls{vampire} construct the \gls{precedence} by sorting the symbols by their arity or number of occurrences in the input problem.
In the work presented below \cite{DBLP:conf/cade/Bartek020},
a \gls{precedence} is constructed using six simple symbol features,
including the arity and the occurrence count.
First, a pairwise preference regressor (Elastic-Net or Gradient Boosting) predicts a preference value for each pair of symbols.
Second, a precedence is constructed using the predicted preference values.
The regressor is trained on runs of Vampire with \gls{kbo} instantiated by random precedences.
The training aims to minimize the time it takes Vampire to solve an input problem.

The main contribution of this work is that it established how pairwise ranking can be used as a proxy task in \gls{ml} for \gls{atping} (in this case, for training a \gls{sp} recommender).
This proved especially useful in my subsequent research,
as presented in \cref{sec:results:npr,sec:results:selection}.

\IncludePaper{publications/simple.pdf}

\section{Neural Precedence Recommender}
\label{sec:results:npr}

Filip Bártek and Martin Suda.
Neural Precedence Recommender.
\Acrlong{cade} 28, 2021.
\cite{DBLP:conf/cade/Bartek021}

\IncludePaper{publications/Neural Precedence Recommender.pdf}

\section{A GNN-Advised Clause Selection}
\label{sec:results:selection}

Filip Bártek and Martin Suda.
How much should this symbol weigh? A \acrshort{gnn}-Advised Clause Selection.
\Acrlong{lpar} 24, 2023.
\cite{DBLP:conf/lpar/Bartek023}

\IncludePaper{publications/weights.pdf}

\section{Regularization in Spider-Style Strategy Discovery and Schedule Construction}
\label{sec:results:regularization}

In the research presented below \cite{DBLP:conf/ijcar/BartekCS24},
my co-authors and I created a system that automatically generates strong and mutually complementary strategies (configurations) for the \gls{atper} Vampire.
Using approximately 1000 strategies generated by this system,
we constructed strong strategy schedules that generalize well to unseen problems.

A greedy algorithm is at the core of the strategy construction process.
The algorithm is parameterized with several regularizing options.
These new variants of the algorithm along with an analysis of their regularizing strength
constitute the main contributions presented in the paper.

A manuscript with additional appendices is available online \cite{DBLP:journals/corr/abs-2403-12869}.

\IncludePaper{publications/regularization.pdf}

\section{Cautious Specialization of Strategy Schedules}
\label{sec:results:cautious}

In \cref{sec:results:regularization},
my co-authors and I focused on constructing a monolithic strategy schedule that performs well on the whole target set of problems.
The performance of the prover can be further increased by dividing the target set of problems into classes
and constructing a specialized schedule for each of the classes.
Each of such classes should be identified by problem features that are easy to compute
so that the branching system of schedules can be used on an unseen problem efficiently.

As long the classes are relatively homogeneous with respect to the performance of the strategies,
this approach is expected to yield an improvement of performance on the training problems.
However, aggressive specialization may lead to a decreased performance on unseen problems.

In the extended abstract presented below \cite{DBLP:conf/paar/BartekC024},
we expanded on the work introduced in \cref{sec:results:regularization}
by investigating the problem of specialization of strategy schedules.
We performed two initial experiments:
\begin{enumerate}
\item First, we compared a hand-tweaked split of the problem space into three classes by the number of atoms to a random split.
\item Second, we experimented with training a collection of boosting trees to effectively tailor a schedule for an arbitrary input problem.
\end{enumerate}

\todo[inline]{Add a punchline. "The most interesting observation was ..."}

\IncludePaper{publications/cautious.pdf}
