\chapter{Preliminaries [to be removed]}
\todo[inline,color=red]{Remove this chapter once its content is subsumed by \cref{sec:sota}.}

\section{Automatic Theorem Proving (ATP)}

Many problems can be defined in \gls{fol}.
Notable examples include problems in mathematics and software verification.
Solving a problem in \gls{fol} amounts to finding a proof that the conjecture necessarily follows from the premises.
A common approach is \defn{refutation-based proving}:
We search for a proof that a trivial contradiction is a logical consequence of the premises conjoined with the negated conjecture.

\Gls{atping} for \gls{fol} has been successfully applied in software verification and advanced mathematics.
Prominent successes include ...\todo{TBA}

\Gls{casc} is a prominent annual \gls{atping} competition.
In the \gls{fof} division, the provers are evaluated on a set of 500\todo{Really?} \gls{fol} theorems.
To win the competition, the prover needs to solve as many theorems as all of its competitors.

\Gls{casc} is closely related to the \gls{tptp} problem library.
A part of the evaluation problems are sampled from \gls{tptp}, and all the problems in \gls{casc} are added into \gls{tptp} after the competition for later reference.
Both \gls{casc} and the \gls{tptp} library use the \gls{tptp} language to define the problems.
The \gls{tptp} language is, in practice, the standard language for the definition of \gls{fol} problems.

\subsection{Saturation-Based Theorem Proving}

A saturation-based \gls{atper} searches for a proof iteratively.
In each iteration, one of the unprocessed clauses is selected and moved to the active clause set.
Then, all the possible inferences within the active clause set are performed.

The process can be improved with forward and backward subsumption.

\subsection{Strategies}

A typical \gls{atper} employs several heuristics that guide the proof search.
The heuristics determine, for example, which clause is selected as the given clause, or which inferences are performed.
Furthermore, the heuristics are typically parameterized, as there is no single configuration that works the best for all problems of interest.
A configuration of all heuristics of a \gls{atper} is known as a strategy.

In the context of \gls{atping}, \defn{\gls{aac}} is the task of finding, given an input problem, a strategy that is expected to solve the problem in a short time.
In general, \gls{aac} can be applied to any parameterized solver.
Several general-purpose \gls{aac} systems have been proposed.

\subsection{Vampire}

\subsection{TPTP Problem Library}

\section{Machine Learning (ML)}

\Gls{ml} automates the process of creating a responsive system with a useful behavior.\todo{Compare with a dictionary definition.}
In supervised \gls{ml}, such a system is trained on a dataset that consists of training examples,
each of which specifies an input to the system and a corresponding desired output (target label).

\subsection{Neural Networks}

Since ..., \glspl{ann} have been a prominent paradigm in \gls{ml}.
The rise of \gls{dl} to prominence was precipitated by the success of \gls{cnn} for image classification.

\Glspl{gnn} use the idea of aggregating local aggregation of information that is at the core of \glspl{cnn}, and generalize it to arbitrary graphs.

\subsection{Algorithm Configuration}

\section{Combining ATP with ML}

The proof search in an \gls{atper} is guided by several heuristics.
These heuristics are parameterized.

For example, the clause selection heuristics decides which of the passive clauses should be selected.

Black-box view: heuristic optimization

\Gls{AxiomSelection}

Clause selection

Simplification ordering on terms

\todo[inline]{Discuss related work (Enigma etc.).}

\Gls{enigma} uses \gls{ml} to train a clause selection heuristic.
First, training data is collected by running proof searches on many problem and collecting their traces.
The clauses inferred in the proof searches are labeled as positive or negative:
The clauses that contributed to the proof are labeled positive, and the remaining clauses are labeled negative.
This training data is used to train a clause selection model that prioritizes positive clauses over negative and, hopefully, generalizes to unseen clauses.

\Gls{ml} systems have been used to improve \gls{AxiomSelection}.
In this approach, the trained model is evaluated before the proof search to prune the premise set.
This way, the completeness of the proving procedure is compromised.
However, in problems with huge premise sets, some kind of \gls{AxiomSelection} is often very helpful and a smart \gls{AxiomSelection} may aid even further.

\todo[inline]{Mention automated algorithm configuration.}


\subsection{Notes on FOL Reasoning [to be removed]}

\todo[inline]{FOL solver is a general-purpose solver. Problems from many domains can be encoded as FOL problems.}

Practical problems from various domains can be translated into general-purpose formal languages,
such as \gls{PropositionalLogic} and \gls{fol}.\todo{What are the limitations of the encodings?}\todo{Demonstrate on examples.}
Deductive reasoning\todo{Unclear that this is related to logic.} offers clear-cut solutions\todo{vague} to formalized\todo{Unclear. Formalization is conversion of a problem in logic.} problems,
and each such solution is justified by a proof.

\todo[inline]{Reasoning task $\approx$ proving validity (semantic entailment) or satisfiability (existence of a model)}

\todo[inline]{Formalization: Solution to the original problem is equivalent to the *validity* (or satisfiability, existence of model) of a FOL formula.

The complex problem is deciding whether a fomula is valid.}

\todo[inline]{FOL is not a language. FOL theorems is a language. Disambiguate throughout the text.}

\Gls{sat} solvers have shown great success and practical applicability in solving problems formulated in \gls{PropositionalLogic} \cite{DBLP:series/faia/336}.
This challenges the intuition that \acrshort{np}-hardness makes \gls{sat} impractically hard.

The \gls{fol} paradigm offers higher expressive power and thus more compact formulations,
which makes it suitable for problems in complex domains such as mathematics and software verification.\todo{Express idea: FOL prover is a general-purpose solver.}\todo{Consider mentioning \gls{hol}.}
However, the expressiveness comes at the cost of higher complexity\todo{vague} of the reasoning task.
%The complexity of \acrlong{fol} is epitomized\todo{} by the undecidability of the logic \cite{}.
Provability in \gls{fol} is undecidable.
% If the input CNF is satisfiable, Vampire may loop forever without saturating the clause set.
% If the input CNF is unsatisfiable, Vampire will necessarily find the proof in a finite number of steps.

A \gls{fol} \defn{problem} is represented by a set of \defn{axioms} and a \defn{conjecture},
each of which is a \gls{fol} sentence.
\todo[inline]{A solver for FOL is a prover. We focus on proving (refutational, entailment) because FOL is hard. Disprovers (model builders etc.) are necessarily incomplete.}
The task of an \gls{atper} is to prove that the axioms entail\todo{Unexpected, baf. "logically entail". Introduce.} the conjecture.\todo{This sentence is unexpected.}
\defn{Refutational} provers reduce this task to proving that the axioms together with the \defn{negated conjecture} are inconsistent.

\todo[inline]{"refutational complete": no need to explicitly qualify "refutational". Vampire is complete in what it does: refutational proving.}

\subsection{Notes}

\todo[inline]{Clarify terms: TPTP, Vampire, search space, superposition-based ATP, parameterized problem solving (?), proof search (as training data point, as process), problem (input problem), \gls{sot}, superposition inference, literal selection, inference, KBO, combinatorial explosion of proof search, heuristics, strategy scheduling, strategy optimization, complementary strategies}

\todo[inline]{Ideas: "ATP is important" - provers, ... MLTP is quite established (?) - AITP conference ...}
\todo[inline]{Discuss prominent successes of machine-assisted proving. Bill McCune w/ Otter - Robbins algebra problem. Sledgehammer. Software verification. Draw inspiration from Urban, Jasmin.}
\todo[inline]{Ensure the introduction reads like a story.}

\todo[inline]{
ATPing has solved many useful problems. It's a cool technology.

Since ATPing is hard, it uses heuristics.

The heuristics are traditionally tuned by humans. ML has been used increasingly to improve ATP performance.

How to do ML for ATP? - Proof search guidance, HPO.
}

\todo[inline]{Justify: \enquote{Vampire represents the state of the art of \gls{atping}.} Justify by CASC report etc.}
