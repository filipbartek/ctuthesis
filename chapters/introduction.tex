%This chapter includes the linking text mandated by Art. 3, par. 2:
%If the dissertation takes the form of a set of publications as per Art. 1, par. 3, instead of
%par. 1 d), it shall comprise a set of no less than three impacted articles, out of which for no
%less than two, the doctoral student is stated as the main author, as per the requirements of
%Art. 12, par. 2 SEC, and par. 1 c) and 1 e) shall be replaced by a linking text in the extent of
%no less than 10 pages.

\chapter{Introduction}

%c) in the introductory part
%i) an overview of the current state-of-the-art in the given field of science (with references to literature) and
%ii) the aims of the dissertation,

%c) v úvodní části
%i) přehled současného stavu dané vědní problematiky (s odkazy na
%literaturu) a
%ii) cíle disertace,

%\epigraph{Then ve jumped to the Truth Mines.}{Greg Egan, Diaspora}
% Page 40

\epigraph{In the Truth Mines, though, the tags weren't just references; they included complete statements of the particular definitions, axioms, or theorems the objects represented. The Mines were self-contained: every mathematical result that fleshers and their descendants had ever proven was on display in its entirety.}{Greg Egan, Diaspora}
% Page 41

%\epigraph{Now that ve'd experienced Truth Mining for verself, Yatima could only agree. There was nothing in any scape or library file, any satellite feed or drone image, more beautiful than mathematics.}{Greg Egan, Diaspora}
% Page 43

\todo[inline]{Search for knowledge, problem solving}

\section{State of the Art}
\label{sec:sota}

\todo[inline]{Too technical. Make it more storylike.}
\todo[inline]{Make terminology more consistent. Representation, solving (original problem or FOL problem), introduce "task" (computational task). Witness, certificate, convince, argue, ascertain $\approx$ proof. ATP finds proof, proves conjecture (don't use "proving" in other meanings).}

\subsection{Advent of First-Order Logic}

Throughout history, mankind has faced a wide array of challenges of analytical nature.
Mathematics rose to prominence as a discipline that studied, on an abstract level, the tools and approaches that demonstrated success in solving such challenges.
Mathematical knowledge grew primarily in the directions motivated by
%Mathematical knowledge grew from the ground provided by
natural sciences, engineering, and social sciences.
At the same time, the fields of mathematics and logic
fascinated humans
as the only rigorous\todo{Differentiate from philosophy and theology.} disciplines that can, at least in theory, be studied in isolation from the material world we inhabit.\todo{Relate to Mathematical Platonism.}

\todo[inline]{Add a paragraph on the history of logic.}

In the 19th century, mathematics was considered a mature and important discipline of human intellectual endeavor.
Meanwhile,
advances in logic\todo{What advances, specifically?} opened new possibilities
for resolving
and clearly delimiting ambiguity in mathematics.
The need for clarity in mathematics
crystallized in the search for formally defined foundations of the discipline as a whole.
In the twentieth century, Zermelo--Fraenkel set theory emerged as the standard foundational theory
and classical \gls{fol} was established as a suitable formalism for expressing the axioms and theorems of the theory \cite{DBLP:journals/bsl/Ferreiros01}\todo{Is this citation relevant enough?}.\todo{This paragraph repeats \enquote{mathematics} many times.}

% Notes from CB:
% ZF (set thory) FOL was the formal foundation.
% Hilbert Bernais 1920s - modern FOL
% Hilbert restricted logic to first-order (what we call so now)
% Standard reference for HOL: Church 1940

%During the 20th century,
%\gls{fol} emerged as the standard formalism for foundations of mathematics
%(namely the set theory)
%\cite{DBLP:journals/bsl/Ferreiros01}.
%\todo[author=MS]{prijde mi, ze ted ta prvni veta kulminuje necim (set theory), o cem pak vlastne vubec nebudes mluvit, coz je trosku divny}
%\todo[author=MS]{jeji zacatek zni tak trochu jako pripominani historie. Pak by se nekdo znaly mohl ptat: A kde jsou Boole, Peirce, Frege (https://plato.stanford.edu/entries/logic-firstorder-emergence/), i kdyz ty jsou prave ukotveni jeste v 19. stol}
%This concluded historical development tracing back to Aristotle's syllogistic logic.
% https://plato.stanford.edu/entries/logic-firstorder-emergence/

In addition to a formal language that allows the expression of complex statements,
\gls{fol} defines unambiguous notions of valid statement and sound reasoning.\todo{Define reasoning. Reasoning is proving validity or satisfiability.}
In practice, these notions have been applied in various subdomains of mathematics, such as arithmetic, group theory, and graph theory.

Although mathematics played a crucial role in the establishment of \gls{fol},
the expressive power of \gls{fol} makes it suitable for reasoning in other analytical domains.
% What is ATP good for: https://tptp.org/Seminars/ATP/Applications/Summary.html
% These are actually example use cases of ATP, not FOL.
\Gls{fol} and its extensions have been applied in, for example, software and hardware verification and synthesis
\cite{
DBLP:journals/tcad/DSilvaKW08, % Survey of automated SW verification
DBLP:series/lncs/10001}, % KeY uses dynamic logic, an extension of FOL.
common-sense reasoning \cite{DBLP:conf/cade/PeaseS07}, and
legal reasoning \cite{DBLP:journals/logcom/PrakkenWBA15,
% ASPIC+ uses FOL extended with defeasible rules.
DBLP:conf/atal/LibalN21}.\todo[color=yellow]{The listed areas are actually related primarily to automated reasoning rather than \gls{fol}.}
% The Legislation Editor NAI uses "first-order Deontic logic".
%These areas demonstrate the general importance of reasoning in \gls{fol}.
\todo[inline,author=MS]{celkove prvni odstavec zatim pusobi nedokoncene; drive jsi tam mel tusim "problems"/"represented in logic"/"solvers" a to smerovani mi proslo nosnejsi. Mozna muzes mit nejdriv historii, pak aplikace (jeste zkust nejake prihodit) a pak to modelovani a resice?}

\subsection{Reasoning in First-Order Logic}

\todo[inline]{Consider specializing this section to \emph{automated} reasoning in FOL. "To make a part of reasoning computable, we use logic.}
\todo[inline]{Introduce \emph{automated} reasoning. Consider discussing advent of computer science.}
\todo[inline]{Compare FOL reasoning to SAT solving.}
\todo[inline]{Why is reasoning in FOL so hard? How hard is it? Discuss undecidability and incompleteness.}

\Gls{fol} is a general-purpose reasoning paradigm.
In this section, I outline how \gls{fol} is used in reasoning,
focusing especially on the notions and approaches relevant for automated\todo{automatic?} reasoning.\todo{This paragraph is too vague. What is reasoning?}
I invite readers interested in more details to consult the relevant literature on \gls{logic} \cite{DBLP:books/daglib/0072413,DBLP:books/daglib/0082098} and \gls{ar} \cite{DBLP:books/daglib/0022394,DBLP:books/el/RobinsonV01}.

In \gls{fol}, a reasoning task is defined by a \defn{signature}, a set of \defn{axioms}, and a \defn{conjecture}.
The signature is a set of non-logical (predicate and function) symbols.
%\footnote{These are contrasted with logical symbols -- logical connectives (such as $\lnot$, $\land$, $\lor$), equality $=$, and quantifiers ($\forall$, $\exists$).}
The axioms and the conjecture are \gls{fo} \defn{sentences} (formulas with no free variable occurrences) over the signature.
The set of axioms constitutes a \defn{theory};
notable theories axiomatizable in \gls{fol} include set theory, group theory, and various fragments of arithmetic.
%\footnote{Predicate symbols map tuples of elements to Boolean values, and function symbols map tuples of elements to elements.}

An \defn{interpretation} of the signature specifies the semantics of each of the non-logical symbols and, by composition, each formula over the signature.
%The (possibly\todo{Or even necessarily?} infinitely many) interpretations that satisfy a set of axioms form the \defn{theory} of the axioms.\todo[author=MS]{"In mathematical logic, a theory (also called a formal theory) is a set of sentences in a formal language." \url{https://en.wikipedia.org/wiki/Theory\_(mathematical\_logic)}}
If the conjecture is satisfied by every interpretation that satisfies the axioms,
%(that is, the interpretation is a model of the theory),
the axioms \defn{logically entail} the conjecture.
% Here we define "theorem" solely for "automated theorem proving" to make more sense.
We then call the conjecture
a \defn{logical consequence} of the axioms,
or a \defn{theorem} of the theory defined by the axioms.\footnote{Strictly speaking, a theorem is a formula provable in some calculus.
Gödel's completeness theorem shows that in \gls{fol},
provability coincides with logical entailment \cite{DBLP:books/daglib/0072413}.}.

%\Gls{fol} is an established formalism capable of representing\todo{Discuss the process of representation in more detail. Give an example.} a wide range of reasoning tasks from diverse areas such as mathematics and software verification\todo{Add more examples.}.
%A \gls{fol} representation of a problem consists of a set of \defn{axioms} and a \defn{conjecture},
%each expressed as a \gls{fol} formula.
%The axioms \defn{logically entail} the conjecture if and only if\todo{Informalize -- use "if" or something similar.}
%every interpretation\todo{unclear term}\todo{Here we implicitly assume that all variables are bound. Otherwise, we would need to talk about valuation too.} that satisfies all the axioms also satisfies the conjecture\footnote{Note that if the conjecture is logically entailed by an empty set of axioms
%(that is, satisfied by every interpretation),
%the conjecture is said to be \defn{logically valid} (tautological).}.
%Solving the problem amounts to verifying that such entailment holds.\todo{Storify, add more detail, expand.}\todo{"holds" and "entailment" are both metalogical technical terms.}
%%(or, equivalently, that the conjecture holds assuming the axioms hold).

Proof by contradiction is a common technique for certifying the validity of a theorem in mathematics.
A direct counterpart exists in formal logic:
The task of certifying
entailment is easily reduced to
certifying the
unsatisfiability of a set of formulas.
The set of axioms $\Gamma$ entails conjecture $C$\todo{Consider using another symbol for conjecture. Try to find a standard one.} if the set of formulas $\Gamma \cup \{\lnot C\}$ (where $\lnot C$ is the negation of $C$) is unsatisfiable, that is, falsified by every interpretation.
In the rest of this text, we call such a set of \gls{fol} formulas an \defn{\gls{fol} problem}.

%One of the approaches of establishing such entailment is proving by contradiction.
%To prove the entailment \defn{by contradiction}\todo{Introduce: Why do we care about proving by contradiction?},
%it suffices to prove\todo{I use "prove" too often.} that the set of all axioms together with the negated conjecture is unsatisfiable\todo{Clear?}.
%We call such representation of a problem \defn{refutational}\todo{Try to use standard terminology instead. Or factor out the term "refutational problem".}.
%%This is the basis of \defn{refutational theorem proving}.

By applying
the De Morgan laws,
% 1. To obtain negation normal form (NNF) by pushing down negations; we also use "invinite De Morgan laws" to push below quantifiers
% 2. To obtain CNF
Skolemizing,
% Skolemization eliminates existential quantifiers. The resulting formula is equisatisfiable but not equivalent.
pulling out quantifiers, and
naming subformulas (Tseitin transformation),
every \gls{fol} problem can be transformed into an equisatisfiable set of universally quantified \defn{clauses} (disjunctions of \gls{fo} literals) in polynomial time \cite{DBLP:books/el/RV01/NonnengartW01}.
We call such a representation of the problem \defn{\gls{cnf}}
and the process leading to it \defn{clausification}.

Solving a \gls{cnf} problem amounts to showing that the set of clauses is unsatisfiable.
The standard way to define a proof of unsatisfiability (or, more generally, entailment) is based on an \defn{inference calculus} -- a set of inference rules.
A proof, then, is a finite sequence of clauses,
each of which is either an input\todo{potentially unclear} clause or a clause derived from some (typically one or two) of the preceding clauses by an inference rule.
If each of the inference rules is \defn{sound},
then each of the clauses in the proof is a logical consequence of the input set of clauses.
Finally, if the empty clause (a trivial contradiction) appears in the proof,
the proof certifies the unsatisfiability of the input set of clauses.
In this paradigm, the task of proving the unsatisfiability of a set of clauses can be realized as a search for (a derivation of) the empty clause.

\defn{Complete} inference calculi are of special interest:
If an inference calculus is complete and the input clause set is unsatisfiable,
then a proof of unsatisfiability exists.
In such a case, a proof (being a finite object) can be found, for example, by
exhaustively
enumerating clauses derivable from the input clauses.
Enumerating the derivable clauses in a way that ensures a fast derivation of the empty clause (provided the input set of clauses is unsatisfiable) is the main goal of saturation-based \gls{tping}.

%If a set of clauses is unsatisfiable, then its negation is logically valid.
%If a set of clauses is satisfiable, than the satisfying interpretation defines a counter-example to the negation of the set.

%The problem of unsatisfiability and, by reduction, entailment in \gls{fol} is semi-decidable:
%There is an algorithm that will, if the input clause set is unsatisfiable, eventually find a proof and therefore establish the unsatisfiability,
%but there is no algorithm that would determine the satisfiability of every satisfiable clause set \cite{}.
%\todo[inline]{For each algorithm that is sound and complete, there is a problem that will make the algorithm loop forever. Any alg. will necessarily not terminate on some input.}
%%This is related to the fact that to show the satisfiability, one has to prove that an interpretation with certain properties exists,
%%while this interpretation may be arbitrarily large (infinite) and complex.

%Given a sound inference calculus, the process of solving a \gls{fol} problem can be viewed as a search for the empty clause.
%The search starts by visiting the input clauses and
%each subsequent step visits a clause derivable by the inference rules from some of the clauses visited before.
%\defn{\Gls{atping}} automates such search.

\subsection{Saturation-Based Theorem Proving}

\Gls{saturation} is the state-of-the-art approach to \gls{fol} \gls{atping},
as demonstrated by the continued success of the saturation-based theorem provers such as Vampire \cite{DBLP:conf/cav/KovacsV13}, E \cite{DBLP:conf/cade/0001CV19}, and SPASS \cite{DBLP:conf/cade/WeidenbachDFKSW09} in the \gls{casc} \cite{Sut16}.\todo{This paragraph breaks the flow. Move it somewhere else.}

A \defn{\gls{saturation}-based \gls{tper}} structures the search for contradiction
(the empty clause)
by incrementally expanding the \defn{processed set}
%\todo{Consider using a different terminology: active and passive set.}
-- a set of clauses that have been used exhaustively as premises of inference rules to derive new clauses -- while consuming clauses from the \defn{unprocessed set} -- a set of clauses that have been inferred from the input clauses and have not been used as premises of inferences yet.
%deriving new facts,
%starting with the input problem expressed in \gls{cnf}.
%\Gls{cnf} is sufficiently expressive:
%Any \gls{fol} problem can be converted (\enquote{clausified}) into an equisatisfiable \gls{cnf} in polynomial time.

The search starts by initializing the processed set as empty and populating the unprocessed set with the input clauses.
In each iteration,
a clause is selected from the unprocessed set.
This clause, referred to as the \defn{given clause}, is moved to the processed set.
Then, all the clauses in the processed set are considered potential partners of the given clause
to act as premises of all the inference rules in the calculus implemented by the prover.
%Then, all the admissible inferences, in which all premises are in the processed set and at least one premise is the given clause, are performed.
The new clauses derived by the inferences are added to the unprocessed set.\footnote{More precisely, if a new clause is subsumed (and thus implied) by another clause derived previously, the new clause may be discarded without loss of completeness. This optimization, known as forward subsumption, is widely used in practice \cite{DBLP:journals/corr/cs-SC-0310056,DBLP:journals/jar/Voronkov95,DBLP:books/daglib/0022394}.}

As soon as the empty clause is derived, the search terminates.
The proof of the unsatisfiability of the input problem consists of the empty clause and all the clauses that contributed to its derivation.
This shows the importance of clause selection for the efficiency of the proof search:
If the prover always selected a clause that ends up in the final proof,
the proof search would be highly efficient in terms of iterations of the saturation loop.
% Not perfectly efficient, since a shorter proof may exist.
However, provers typically derive many clauses that do not contribute to the proof,
which makes the clause selection heuristic an attractive target for optimization.

The calculus plays a crucial role in the proof search -- it specifies which inferences are available in each iteration.
If the calculus is prolific (allowing relatively many inferences on a set of clauses), the proof search may slow down to a crawl quickly as the number of clauses derived in each iteration increases.
For this reason, a successful inference calculus restricts the inferences as much as possible while maintaining completeness.

For example, the superposition calculus \cite{DBLP:journals/logcom/BachmairG94}
is parameterized by a \defn{literal selection function} -- a function that selects a subset of literals in each clause.
Inferences are only made on selected literals.
If the selection function is well-behaved,
i.e., it selects either a negative literal or all literals that are maximal with respect to a \gls{to},
% The term ordering must be a fixed simplification ordering.
the restriction preserves the completeness of the calculus.
% Note: Other restriction may preserve completeness too.
In certain scenarios, it is beneficial to further restrict the inferences in a way that forfeits completeness
in exchange for a narrower and faster-converging proof search.

%\todo[author=MS]{technicky potrebujes v superposition (jako kalkulus) nejen superposition (jako rule), ale i equality factoring a equality resolution (jako dalsi rules)}.

% Superposition is a generalization of Knuth-Bendix completion.
% Superposition on UEQ is unfailing KB completion.

In summary, two main decision points guide the \gls{saturation}-based proof search:
\begin{enumerate}
\item Clause selection: Which of the unprocessed clauses should be selected?
\item Inference restriction: Which of the inferences should be applied to the selected clause?\todo{Precedences actually likely work by pruning the search space rather than guidance.}
% \todo{This is affected by \gls{sot} (literal selection, equality orienting) and forward subsumption.}
\end{enumerate}
%My research has dealt with both of these:
%See \cref{sec:contrib:ClauseSelection} and \cref{sec:contrib:SymbolPrecedenceRecommenders}, respectively.

In addition to these, the performance of a saturation-based prover may also depend on the configuration of the preprocessing of the input problem (classification, \gls{AxiomSelection} \cite{DBLP:conf/cade/HoderV11}), redundancy elimination (forward and backward subsumption), and other features, such as the integration of a \gls{sat} solver \cite{DBLP:conf/cav/Voronkov14}.

%Besides these, the performance of a saturation-based prover typically depends on a number of heuristics that control preprocessing (clausification) and the choice of the inference calculus and further restrict the inferences.\todo{Improve.}
%Such heuristics can be configured by various command-line options,
%and the complete configuration specifies a proof search \defn{strategy}.
%This dissertation thesis discusses various ways of adjusting the heuristics in the prover Vampire using \gls{ml}.

\subsection{Machine Learning for Theorem Proving}

\todo[inline,color=green]{Reference \href{https://github.com/IBM/TRAIL}{TRAIL}.}

%\todo[inline]{Describe all the crucial parameterized heuristics in Vampire.}

%Various approaches have been used to assess the performance of \glspl{atper}.
%The most common ones are success rate under a fixed time limit and \gls{par} \cite{}.
%Both of these approaches are parameterized by a set (or, more generally, a distribution) of input problems.
%In general terms, we want the prover to solve problems of interest in a relatively short runtime.

Each of the main decision points in a theorem prover is governed by a \defn{heuristic} -- a procedure that aims to resolve the decision so as to optimize the performance of the prover.\footnote{Standard performance measures include success rate and \gls{par} \cite{DBLP:journals/ai/BischlKKLMFHHLT16}. Each of them is parameterized by a runtime limit and a set of input problems.}
%\footnote{The most common measures of performance of a prover on a set of problems are success rate and \gls{par} \cite{}. Both of them assume a fixed runtime limit.}
%\todo{Too technical?}
Many such heuristics can be configured to specialize the prover to a particular kind of input problems.

For example, clause selection is often implemented by interleaving two priority queues (\enquote{age} and \enquote{weight}).
The ratio in which these queues are interleaved, the so-called \enquote{age-weight ratio}, is configurable.
There is no universally best value of the ratio;
the optimum value depends on the input problem
%\footnote{Specifically, in the unit problems (all clauses are unit) with equality,
%weight-based selection should be performed twice as often as age-based selection,
%while in problems with at least one non-Horn clause,
%weight-based selection should be performed 15 times as often as age-based selection.}
\cite{DBLP:conf/cade/SchulzM16}
%\todo{Schulz doesn't claim the difference is significant so it may be just due to noise. I would like to assume it's significant because it reaches more than \pc{10} solved problems when comparing two classes of problems (unit eq and non-Horn eq).}
% unit eq: FIFO is relatively good. general eq: FIFO is relatively bad.
% In unit eq, the weight-age ratio 2 is better than 15, solving 10.22 % problems.
% In general eq, the weight-age ratio 15 is better than 2, solving 0.89 % problems.
% > In the unit-equality category, FIFO is comparatively much weaker than in the other categories.
% https://docs.google.com/spreadsheets/d/1tcoeSrXt1wo3RnUJ4uqtvrO79sue8FZSV6xOeWaipmY
and, in general, on the configuration of the other heuristics.
%\todo[author=FB]{I cannot find any source to cite on this.}

Traditionally, finding a good configuration of one or more heuristics\todo{Foreshadow strategy scheduling in this section.} in a prover is the work of an expert user that draws from their experience with \gls{atping}.
% Note: Here we use the generic pronoun "they". This is recommended by APA Style: https://apastyle.apa.org/style-grammar-guidelines/grammar/singular-they
Modern computing technology has opened new possibilities for the automation of expert decisions in various domains by \defn{\gls{ml}} \cite{DBLP:books/lib/HastieTF09,DBLP:journals/nn/Schmidhuber15}.
% Hastie provides an overview of machine learning (as does Mohri).
% Schmidhuber argues that GPUs have made NNs much more powerful.
%and a growing number of researchers believes in the potential of applying \gls{ml} to \gls{atping}.
However, to apply \gls{ml} to \gls{atping}, specific considerations\todo[color=green]{What are these considerations? Does the text make them clear?} must be taken into account.

\todo[inline]{"statistical models" "data from prover runs" - what could it mean to use ML for ATP? Foreshadow the challenges -- representation of formulas and terms, etc.}

For example, we could try to predict, for an arbitrary input problem, an age-weight ratio
that makes the prover likely to solve the problem in a given time limit.
To train such a system using standard supervised learning techniques,
%Let us consider the example of clause selection using the age- and weight-based priority queues once more.
%To train a \gls{ml} system that predicts a good value of the age-weight ratio for an arbitrary input problem,
we would need to map input problems to numeric vectors of fixed length.
For example, we could construct the numeric vector representation using human-designed features,
such as symbol counts or term walk counts \cite{DBLP:conf/mkm/JakubuvU17},
or automate feature extraction using a \gls{rnn} \cite{DBLP:conf/cade/ChvalovskyJ0U19,DBLP:conf/iclr/EvansSAKG18} or a \gls{gnn} \cite{DBLP:conf/cade/JakubuvCOP0U20}.

Since the satisfiability of a set of clauses does not depend on the particular naming of the symbols and variables,
we may decide to use an input problem representation that abstracts away from the symbol and variable names.
This ensures we are learning general patterns based on common abstract sub-structures in the problems,
rather than idiosyncratic patterns based on naming conventions used by the experts who encoded the problems.
\Glspl{gnn} support such naming-agnostic representation naturally \cite{DBLP:conf/cade/JakubuvCOP0U20}.

%We may include the names of some or all the symbols and variables,
%or leave them out to make the system oblivious to the symbol and variable names,
%relying solely on the logical structure of the problem.
% Actually, we typically also provide roles of the clauses.
%Furthermore, we need to decide what data the system will be trained on
%(for example, outcomes of prover runs with varying age-weight ratio and input problem).
Our final goal is to optimize the performance of the prover in terms of, for example, the success rate on an input problem set with a fixed per-execution time limit.
For training purposes,
we typically need to approximate this measure by a \defn{proxy task} -- an optimization task that admits \gls{ml} and is believed to correlate with the final performance measure.

For example, the task of estimating a good age-weight ratio can be implemented as a supervised learning task, provided that we have a ground truth available
in the form of an assignment of age-weight ratios to training problems.
To obtain such assignment, we could, for example, evaluate each training problem with several age-weight ratios and assign the best of the observed values to the problem.
This simplification establishes a proxy task:
Predicting an age-weight ratio close to the best observed value is a machine-learnable proxy of solving the problem.% (in the sense of finding a proof of unsatisfiability).

\todo[inline]{This section goes into a lot of detailed examples. It may be inconsistent with the other sections.}

\todo[inline]{Stress the challenge of ML.}
\todo[inline]{Consider discussing RL and looping here. With RL, we can use the prover as a black box, training on just the output status or runtime, with no introspection. This is done notably in SMAC.

Maybe: Reinforcement learning could do without proxy tasks. "The advantage of RL is ... and the advantage of supervised ... is ..."

But: ML system without a proxy task would effectively output a proof.

Or: Do not mention RL at all.}

%\Gls{rl} might be used to avoid the need for a proxy task
%and train a \gls{ml} system directly on the output of the prover,
%without the need for a proxy supervised learning task.
%Notably, such approach has been used in the general-purpose algorithm configurator \acrshort{smac} \cite{DBLP:conf/lion/HutterHL11}.
%Such approach has been used in the context of connection proving \cite{DBLP:conf/nips/KaliszykUMO18}.

As an alternative to configuring one or more heuristics before the proof search starts (offline integration),
a trained \gls{ml} system may be integrated within the proof search procedure as a replacement for a man-made heuristic (online integration).\todo{What is the source of the "offline" and "online" terminology?}
For example, \gls{ml}-based systems have been trained to perform clause selection directly \cite{
% ENIGMA
DBLP:conf/mkm/JakubuvU17,DBLP:conf/cade/ChvalovskyJ0U19,DBLP:conf/cade/JakubuvCOP0U20,
% Loos
DBLP:conf/lpar/LoosISK17,
% Deepire
DBLP:conf/cade/000121a,DBLP:conf/frocos/Suda21}.
Such \gls{ml} systems evaluate all the inferred clauses to prioritize the most promising ones.

While a growing body of research on \gls{ml} for \gls{atping} exists \cite{DBLP:conf/birthday/BlaauwbroekCGJK24},
%\cite{DBLP:conf/gcai/Urban15,DBLP:conf/cpp/JakubuvU17,DBLP:conf/gcai/SchaferS15,DBLP:journals/jar/KuhlweinU15,DBLP:conf/mkm/HoldenK21}
many avenues remain unexplored.
The work presented in this thesis explores several novel approaches, as detailed in the next section.
%An interesting commonality of the approaches to proof search guidance (\cref{sec:contrib:SymbolPrecedenceRecommenders,sec:contrib:ClauseSelection}) is that they use ranking of objects (symbols or clauses, respectively) as the proxy task.\todo[color=green]{The last sentence feels superfluous, out-of-place.}
%\cref{sec:results:simple,sec:results:npr,sec:results:selection}

% Goal of this section (also): Foreshadow what has not been done yet to prepare ground for contributions.

% Purpose: zoom in on various places of my reserach, convince the reader that it is cool and important and interesting

%In this thesis, we present novel approaches for configuring \gls{to}\todo{SOT is introduced later.} (\cref{sec:results:simple,sec:results:npr}) and clause selection (\cref{sec:results:selection}).
%Special care had to be taken for the trained \gls{ml} system to propose problem-dependent symbol permutation (precedence) or symbol weights, respectively.\todo{Explain in more detail. Expand to a paragraph. At least one of these challenges. "NNs work with vectors of fixed length." "Aggregate experience across different problems."}
%
%In \cref{sec:results:regularization,sec:results:cautious},
%I present research that extends on the task of optimizing all the parameters (the whole strategy) of Vampire jointly.
%To achieve greater performance, several strong and mutually complementary strategies were identified and combined into strategy schedules.
%The main innovation in this part of my research is the systematic treatment of generalization of the strategy schedules\todo{vague}.\todo{Consider discussing strategies separately. New section. "Strategy schedules are also very important for ATP. "No ML model is executed in preprocessing or runtime."}

%A promising approach to automated optimization of the heuristics is based on learning from proof search traces -- sequences of annotated clauses derived in a proof search.
%Standard techniques of \gls{ml} are applicable.\todo{Such formulation undermines the challenge. We should stress we need non-standard techniques. Remember we often used proxy tasks. Another challenge: ML uses real vectors while we use discrete stuff, for example representations of formulas (graphs).}
%For example, clause selection can be cast as a supervised classification problem \cite{}.

%\Gls{ml} has been successfully applied to improve the performance of various \glspl{atper} \cite{DBLP:conf/birthday/BlaauwbroekCGJK24}.\todo{Undermines novelty. It is an open area.}
%Specifically, \gls{ml} has been used to improve axiom selection \cite{}, clause selection \cite{}, ...
%My work adds more insights to this body of research.

%\Gls{gnn} is a \gls{nn} that operates on a graph-structured input, rather than the common tabular, sequential, or image data.
%\Gls{gnn} is especially suitable for \gls{atping} because the input problems and clauses have graph structure, so they can be represented naturally as input of a \gls{gnn}.
%\Cref{sec:results:npr,sec:results:selection} discuss two applications of \glspl{gnn} to \gls{atping}.

\todo[inline]{Explain GNN. Relate to the challenge of representing the logic formulas for NN.

Consider including an overview of proxy tasks -- clause classification, ...}

%\subsection{Strategies and Strategy Schedules}
%
%The union of all the heuristics that control the proof search in a prover constitutes a proof search \defn{\gls{strategy}} \cite{DBLP:books/daglib/0002958}\todo{Should I really cite Schulz as coining the term "strategy"?}.
%Similarly to the individual options,
%the strategies are also specialized to different problems.
%Running several strategies with short time limits in a sequence is often a good way to deal with the uncertainty of which strategy to use.
%Such sequence of strategies with associated time limits constitutes a \defn{\gls{StrategySchedule}}.

%\subsection{Strategy Schedules}

%\subsection{Strategies}
%
%A typical \gls{atper} employs several heuristics that guide the proof search.
%The heuristics determine, for example, which clause is selected as the given clause, or which inferences are performed.
%Furthermore, the heuristics are typically parameterized, as there is no single configuration that works the best for all problems of interest.
%A configuration of all heuristics of a \gls{atper} is known as a strategy.
%
%In the context of \gls{atping}, \defn{\gls{aac}} is the task of finding, given an input problem, a strategy that is expected to solve the problem in a short time.
%In general, \gls{aac} can be applied to any parameterized solver.
%Several general-purpose \gls{aac} systems have been proposed.

\todo[inline,color=yellow]{What is the state of the art in strategy scheduling? Should we describe it for the sake of completeness? MS: Not necessarily. Consider adding a section.}
\todo[inline]{Mention algorithm configuration in \cref{sec:sota}.}

\section{Contributions}
\label{sec:contributions}

\todo[inline]{Consider describing the goals of the thesis in a chapter.}

\todo[inline]{Relate to challenges and proxies outlined above.}

I designed and implemented several software systems that optimize the performance of the saturation-based \gls{atper} Vampire.
Each of these systems employs a novel approach to learn from Vampire executions.

The results of my research have been peer-reviewed and published at various conferences and workshops.
\Cref{sec:results:simple,sec:results:npr,sec:results:selection,sec:results:regularization,sec:results:cautious}
present a selection of these publications.
Each of the publications is included in its final published form and is self-contained.

\subsection{Common Considerations}

Although the experiments presented in the following have been performed using the \gls{atper} \gls{vampire},
the core techniques are applicable to any \gls{saturation}-based \gls{atper}.
Furthermore, these techniques are potentially useful in other solvers in domains both within and outside of logic.\todo{This is never discussed in the thesis nor in the papers. Consider demonstrating on the (prominent) example of strategy schedule construction.}
%\footnote{This is especially true if the solver in question is parameterized by
%a permutation (\cref{sec:contrib:SymbolPrecedenceRecommenders}) or a numeric vector (\cref{sec:contrib:ClauseSelection}) whose length depends on the input (problem) instance, or
%executable in a configuration schedule (\cref{sec:contrib:schedules}).}

In all of the research presented in this thesis, I used the problems in the \gls{fol} fragment of \gls{tptp}\todo{Consider discussing the relation between CASC and TPTP somewhere.} \cite{Sutcliffe2017}, a library of benchmark problems.
The problems in \gls{tptp} represent several target domains of \gls{atping},
such as mathematics and software verification, and have been encoded by various procedures.
For these reasons, they do not have a common \gls{signature} -- a symbol may represent different concepts in different problems, and a concept may appear under different names.\todo{Include examples.}
This motivated some of the design decisions in my research, namely the preference for signature-agnostic problem representations.

Since \gls{vampire} represents the state of the art in \gls{atping} \cite{casc-j12,DBLP:journals/aicom/SutcliffeD23}
and since \gls{tptp} represents various domains of reasoning,
the results can be understood as exploring and expanding the limits of \gls{fol} \gls{atping} in general.
%I focused on \gls{fol} \gls{atping} because this area is well established yet nontrivial.\todo{Write a better justification.}
In all cases, the \gls{ml} is based on learning from the results of executions of the target prover (\gls{vampire}) on problems from the target domain (\gls{fol} \gls{tptp}).\todo[color=green]{rewritten two paragraphs}

\subsection{Symbol Precedence Recommenders}
\label{sec:contrib:SymbolPrecedenceRecommenders}

Vampire uses the superposition calculus \cite{DBLP:journals/logcom/BachmairG94}.
The calculus is parameterized by a \gls{sot} \cite{DBLP:conf/cav/KovacsV13}\todo{Consider citing Term rewriting and all that.},
which restricts the inferences in two ways:
\begin{enumerate}
\item Literal selection: In each clause, inferences are restricted to the selected literals.
Either one of the negative literals
or all literals that are maximal with respect to the \gls{to} are selected.
\item Equation orienting: The superposition inference rule conceptualizes the idea of rewriting terms by replacing equals by equals.\todo{This doesn't sound like my English.}
The \gls{to} orients some of the equations, and each oriented equation is only used as a rewrite rule in one of the two possible directions.
Informally, complex terms are replaced by simple ones.\todo[color=green]{new sentence}
\end{enumerate}
Both of these restrictions effectively prune the proof search without rendering it incomplete.
Choosing a good \gls{to} for a given input problem
can make the pruning more aggressive while still allowing a short proof\todo[color=green]{Is this a good description of what we want to happen?},
which helps the saturation procedure to find a proof quickly.

\Gls{kbo}, the default \gls{to} scheme\todo{Say something more about the ordering. Why is it a good choice?} implemented in \gls{vampire}, is parameterized by a \defn{\gls{sp}} -- a permutation of the \gls{signature} of the input problem.
I explored two approaches to automatically finding a good \gls{sp} to instantiate \gls{kbo} with for an arbitrary input problem.

In my initial paper \cite{DBLP:conf/cade/Bartek020} (see \cref{sec:results:simple}),
I present a \defn{\gls{SymbolPrecedenceRecommender}} that orders the symbols based only on six syntactic symbol features\todo{Stress the challenge of designing the proxy task. "We present a p. recommender which uses ... over syntactic features.}.
In a follow-up work \cite{DBLP:conf/cade/Bartek021} (see \cref{sec:results:npr}),
I improved on this by training a \acrshort{gnn}-based \glslink{SymbolPrecedenceRecommender}{precedence recommender}.
In both cases, I trained the recommender to generate a \gls{sp} that
approximately minimizes the number of iterations the saturation loop takes to solve the input problem.
% simple: cost(\pi) is the number of saturation iteration loops on this precedence. For a new problem, we minimize a lossy estimate of cost.
% npr: \pi < \rho iff \pi solves the problem in fewer saturation iteration loops.
This typically corresponds to
a low \gls{wallclock} \gls{runtime} needed to solve the problem and
a relatively high probability of solving the problem under an arbitrary time limit.

\todo[inline]{Explain somewhere: We learn across problems. We try to generalize across problems.}

%The problems in \gls{tptp}, in general, do not share \glspl{signature} -- one symbol may represent different concepts in different problems, and one concept may appear under different names.\todo{Include examples.}
%This poses an unusual design challenge\todo{Clarify what the challenge is: there is no clear mapping of symbols and the signature sizes vary. Thus we use a signature-agnostic model/system.},
%which eventually led us to apply techniques that have been previously explored in the context of \gls{ltr}\todo[color=yellow]{Is the inclusion of LTR really motivated by signature agnosticism? Wouldn't we use LTR even with aligned signatures? Cite a source on LTR.}.
%Our work connects the fields of \gls{ltr} and \gls{atping} and provides, notably, an instructive description of a \acrshort{gnn}-based permutation recommender.
\todo[inline]{Discuss the connection of my work to LTR somewhere.}

\subsection{Clause Selection Guidance}
\label{sec:contrib:ClauseSelection}

%In each iteration of the main loop,
%a saturation-based \gls{atper} selects one of the clauses that have been inferred so far.
%The selected clause is then added to the active clause set and all the possible inferences within the active set are performed.
%The \glspl{atper} rely on heuristics to select the (given) clause.

Clause selection is, arguably, the most important choice point in a saturation-based prover.
The symbol-counting heuristic \cite{DBLP:conf/cade/SchulzM16}, which is one of the most common clause selection heuristics,
prioritizes clauses that are small in the number of symbol and variable occurrences.
In the weighted variant of this heuristic \cite{E-manual},
the user specifies a weight for each of the symbols
and
the clause weight is computed as the weighted sum of the symbol occurrence counts.

In the work included in \cref{sec:results:selection} \cite{DBLP:conf/lpar/Bartek023},
I modified Vampire to support the weighted symbol-counting clause selection heuristic
and trained a \gls{gnn} to propose a weight for each of the symbols of any given input problem.
%These weights were then used in a weighted symbol-counting clause selection heuristic in \gls{vampire}.
Thanks to the use of a \gls{gnn}, this symbol weight recommender is signature-agnostic,
which overcomes the challenge of misaligned signatures present in \gls{tptp}.\todo{This is common with precedence recommender.}
The final evaluation showed that
the trained heuristic solves \pc{6.6} more problems than the baseline.

%The resulting system solved \pc{6.6} more problems than the baseline,
%which is an improvement greater than that introduced, under the same conditions, by AVATAR,
%a powerful proving technique fusing a SAT solver with FOL ATP.
%Our experiments revealed that constraining the training to assign a weight of at least 1 to each symbol is a fruitful strategy\todo{Disambiguate from the term "strategy"?}.
%The trained system assigned a relatively low weight to the variable occurrences,
%which matches the intuition that general clauses should be preferred to concrete ones.

\subsection{Strategy Construction and Scheduling}
\label{sec:contrib:schedules}

The prominent \glspl{atper} expose various options that control the preprocessing and the proof search.
In addition to \gls{sot} and clause selection we discussed above,
the options configure \gls{AxiomSelection}, subformula naming, redundancy elimination, etc.
% redundancy elimination ~ backward and forward subsumption
The configuration of all of the options of a prover constitutes a \defn{strategy}.
Different problems may require different strategies -- for example, aggressive \gls{AxiomSelection} is, in practice, necessary to solve problems with many axioms, while it rarely brings any benefit for small problems.

The research presented in \cref{sec:results:regularization,sec:results:cautious} combines two important tasks
that are relevant for \gls{atping} as well as for forms of parameterized problem solving:
%in the automation of parameterized problem solving and applies them to \gls{atping}:
\begin{enumerate}
\item \Gls{aac} \cite{DBLP:journals/jair/SchedeBTWBHT22} is the task of finding a good configuration (strategy) for a distribution of problems.
\item Budgeted \gls{AlgorithmScheduling} \cite{DBLP:journals/ec/KerschkeHNT19} is the task of finding a good static algorithm schedule (an assignment of runtime limits to algorithms), given a distribution of problems, a portfolio of algorithms, and a runtime budget.
\end{enumerate}

In our initial work on the topic \cite{DBLP:conf/ijcar/BartekCS24} (see \cref{sec:results:regularization}),
my co-authors and I generated a set of strong and mutually complementary strategies for the \gls{atper} \gls{vampire}
and combined these strategies into strong schedules.
To ensure good generalization of the resulting schedules,
we introduced novel regularization techniques into the scheduling process.
To the best of our knowledge, this is the first principled treatment of generalization of \gls{AlgorithmScheduling}.\todo{Polish the wording.}

In a follow-up work \cite{DBLP:conf/paar/BartekC024} (see \cref{sec:results:cautious}), we investigated the possibility of specializing the schedules to homogeneous classes of problems.\todo{Outline the main result.}

\todo[inline]{Consider including a list of publications in Contributions.}
